{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Compared to previous version\n",
        "  # multiple head attention (vs single attention head in previous version)\n",
        "  # positional encoding (wasn't implemented in previous version)\n",
        "  # multiple transformer blocks (vs single block)\n",
        "  # larger embedding dimention (512 vs 256)\n",
        "  # nonlinearity in transformer's feedforward part + 2 linear layers instead of 1\n",
        "  # pre-LayerNorm which is more common in LLMs (vs post LayerNorm in previous version)"
      ],
      "metadata": {
        "id": "v8FALus5RRQD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHZS4xvQizjO",
        "outputId": "b91b81e7-253c-49be-da1b-887abe349fbe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PcRB4TKaKx3",
        "outputId": "d811c938-d327-434f-a45e-c6db75d2c90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-24 22:59:39--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5436475 (5.2M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   5.18M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-08-24 22:59:42 (367 MB/s) - ‘shakespeare.txt’ saved [5436475/5436475]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-UmyQ7L1wN",
        "outputId": "20a5eda3-b4e5-427a-9f77-f76d83f5c7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 124185  899588 5436475 shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -lwc shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "XCgP9odYasbv",
        "outputId": "157c491e-23d6-4a9c-f301-b9de23854018"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5436475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-e\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "print(len(chars))\n",
        "\n",
        "ctoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itoc = {i:ch for ch, i in ctoi.items()}\n",
        "\n",
        "assert len(ctoi) == len(itoc)\n",
        "\n",
        "\n",
        "def encoder(text):\n",
        "  return [ctoi[ch] for ch in text]\n",
        "\n",
        "def decoder(tokens):\n",
        "  return \"\".join([itoc[token] for token in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5ngsU8bnBu",
        "outputId": "fd79941b-ff5a-4c5a-d2e4-3b0e9cb2d708"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"hello world!\"\n",
        "assert test_str == decoder(encoder(test_str))"
      ],
      "metadata": {
        "id": "t47ofAiIcXft"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_size = 0.9\n",
        "\n",
        "num_epochs = 5\n",
        "batch_size = 1024 + 512\n",
        "emb_dim = 512\n",
        "num_heads = 16\n",
        "num_blocks=4\n",
        "lr = 2e-3\n",
        "context_window_size = 128\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data, test_data = encoder(data[:int(train_size*len(data))]), encoder(data[int(train_size*len(data)):])\n",
        "train_data_t, test_data_t = torch.tensor(train_data, dtype=torch.long).to(device), torch.tensor(test_data, dtype=torch.long).to(device)\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6_56Iba-Ez",
        "outputId": "e61ac19f-a93c-4368-ddea-71ee8108757a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4892827, 543648)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), type(train_data_t), type(test_data), type(test_data_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldkde6lcfdGX",
        "outputId": "85462a6e-ba16-47e6-bc97-6eaa37755fc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, torch.Tensor, list, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(len(train_data)/batch_size*0.9)  # 0.9 multiplier is an scrappy way of making sure get_batch doesn't go out of bounds\n",
        "num_test_batches = int(len(test_data)/batch_size*0.9)\n",
        "\n",
        "print(f\"{num_batches=}, {num_test_batches=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6N-nyrzP3L",
        "outputId": "185af9b3-9dd2-4850-d01c-ea3c05b04d9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches=2866, num_test_batches=318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVnXU2vxBhQ2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# types are explicitly not included\n",
        "\n",
        "# def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "#   # QKV are of shape [batch_size, seq_len, d_k]\n",
        "#   # mask is a tensor of shape [seq_len, seq_len]\n",
        "#   scores = (Q @ K.transpose(-2, -1)) / Q.shape[-1]**0.5\n",
        "#   # K.transpose(-2, -1) shape = [batch_size, d_k, seq_len]\n",
        "#   # Q @ K^T shape = [batch_size, seq_len, seq_len]\n",
        "#   if mask is not None:\n",
        "#     scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "#   return F.softmax(scores, dim=-1) @ V  # shape = [batch_size, seq_len, d_k]\n",
        "\n",
        "\n",
        "def causal_mask(seq_len):\n",
        "  # create a lower trianguar mask\n",
        "  return torch.tril(torch.ones(seq_len, seq_len)).bool()  # shape = [seq_len, seq_len]\n",
        "\n",
        "\n",
        "# class SelfAttention(torch.nn.Module):\n",
        "#   def __init__(self, emb_dim=emb_dim):\n",
        "#     super().__init__()\n",
        "#     self.emb_dim = emb_dim\n",
        "#     self.q_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "#     self.k_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "#     self.v_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "#     self.output = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "#   def forward(self, X):\n",
        "#     # X shape = [batch_size, seq_len, emb_dim]\n",
        "#     Q = self.q_proj(X)\n",
        "#     K = self.k_proj(X)\n",
        "#     V = self.v_proj(X)\n",
        "\n",
        "#     seq_len = Q.shape[1]\n",
        "#     mask = causal_mask(seq_len).unsqueeze(0).to(device)\n",
        "\n",
        "#     return self.output(scaled_dot_product_attention(Q, K, V, mask=mask))\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "    super().__init__()\n",
        "    assert emb_dim % num_heads == 0  # emb_dim is divisible by num_heads\n",
        "    self.q_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.k_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.v_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.output = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.emb_dim = emb_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = emb_dim // num_heads\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "\n",
        "    Q = self.q_proj(X)\n",
        "    K = self.k_proj(X)\n",
        "    V = self.v_proj(X)\n",
        "\n",
        "    # split into heads and change shae [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    # scaled dot product attention\n",
        "    scores = Q @ K.transpose(-2, -1) / (self.head_dim**0.5)\n",
        "    mask = causal_mask(seq_len).unsqueeze(0).unsqueeze(0).to(device)  # shape = [1, 1, seq_len, seq_len]\n",
        "    scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "    attn = F.softmax(scores, dim=-1) @ V  # shape = [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "    # concat heads\n",
        "    attn = attn.transpose(1, 2).reshape(batch_size, seq_len, self.emb_dim)\n",
        "    return self.output(attn)\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=emb_dim):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(emb_dim=emb_dim, num_heads=num_heads)\n",
        "    self.ffn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(emb_dim, emb_dim*4),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(emb_dim*4, emb_dim)\n",
        "    )\n",
        "    self.ln1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.ln2 = torch.nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    res = X\n",
        "    X = self.ln1(X)\n",
        "    X = res + self.attention(X)\n",
        "\n",
        "    res = X\n",
        "    X = self.ln2(X)  # pre-LN\n",
        "    X = res + self.ffn(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, vocab_size=len(chars), emb_dim=emb_dim, seq_len=context_window_size, num_blocks=num_blocks):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    self.positional_emb = torch.nn.Embedding(seq_len, emb_dim)\n",
        "    self.transformer = torch.nn.ModuleList([TransformerBlock(emb_dim=emb_dim) for _ in range(num_blocks)])  # note: using nn.Sequential([TB]) will reference the same instance of TB -> all of them will share the same weights which is not what we want\n",
        "    self.linear = torch.nn.Linear(emb_dim, vocab_size)\n",
        "    self.softmax = torch.nn.Softmax(-1)\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_len = seq_len\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    # X shape = [batch_size, seq_len]\n",
        "    emb = self.embedding_layer(X)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    positions = torch.arange(emb.shape[1]).unsqueeze(0).to(device)  # shape = [batch_size, seq_len]\n",
        "    pos_emb = self.positional_emb(positions)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    x = emb + pos_emb\n",
        "    for block in self.transformer:\n",
        "      x = block(x)\n",
        "    logits = self.linear(x)\n",
        "    if y is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      # during training\n",
        "      return logits, self.loss(logits.view(-1, self.vocab_size), y.view(-1))  # bug fix from previous version. CrossEntropy requires logits not probabilities\n",
        "\n",
        "  def generate(self, prompt=\"\"):\n",
        "    # assuming len(prompt) < seq_len\n",
        "    # TODO: truncate if not\n",
        "    X = torch.tensor(encoder(prompt), dtype=torch.long).to(device)\n",
        "    X = X.unsqueeze(0) # to convert shape to [batch_size, seq_len]\n",
        "\n",
        "    while X.shape[1] < self.seq_len:\n",
        "      logits, _ = self(X)\n",
        "      probs = self.softmax(logits)\n",
        "      next_token = torch.multinomial(probs[0][-1], 1)\n",
        "      X = torch.cat((X,next_token.unsqueeze(0)), dim=1)\n",
        "      # TODO: stop on end token...\n",
        "    return decoder(X.squeeze(0).tolist())\n",
        "\n",
        "\n",
        "model = Model().to(device)"
      ],
      "metadata": {
        "id": "Cc1t8EEPcK9A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num of trainable parameters in the model:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAqE2XT0yl0",
        "outputId": "7d80d70b-9732-490a-8cf6-1e997740745d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12761172"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"Hello w\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nhjCd-AddhJQ",
        "outputId": "2cd4d6aa-a782-41c4-9fa8-39f2fcd7c415"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello w  [IG>vCpmP`JG93t:wbU[|s`1Hg\"kK93fnGm_)\\n35k5ca6emprNmz8GzR`Vh-\\nZYD([]PrtjKbo 4\"JAWuJ0_f\\nhROsz}zX?([vRzyqgur\\nEr-9s71u8hFgI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = train_data_t[:-1], train_data_t[1:]\n",
        "test_X, test_y = test_data_t[:-1], test_data_t[1:]\n",
        "\n",
        "def get_batch(X, y, idx, batch_size=16):\n",
        "  batch_x, batch_y = [], []\n",
        "  for i in range(batch_size):\n",
        "    batch_x.append(X[idx*batch_size+i  :idx*batch_size+i+context_window_size])\n",
        "    batch_y.append(y[idx*batch_size+i+1:idx*batch_size+i+context_window_size+1])\n",
        "\n",
        "  return torch.stack(batch_x), torch.stack(batch_y)"
      ],
      "metadata": {
        "id": "rPAGeQkjjeZn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YL_VrKlAz762"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def eval_model():\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for i in tqdm(range(num_test_batches)):\n",
        "      X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "      _, loss = model(X, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss /= num_test_batches\n",
        "  return test_loss\n",
        "\n",
        "test_loss = eval_model()\n",
        "print(f\"random weight {test_loss=}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    X, y = get_batch(train_X, train_y, i, batch_size)\n",
        "    _, loss = model(X, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  epoch_loss /= num_batches\n",
        "\n",
        "  test_loss = eval_model()\n",
        "\n",
        "  print(f\"{epoch=}, {epoch_loss=}; {test_loss=}\")"
      ],
      "metadata": {
        "id": "S63ht-ZIkT9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"Hello w\")"
      ],
      "metadata": {
        "id": "9LUOOLzsqgjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89204f47-6a04-471a-84e4-483959f639e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello wsad efeyu rwotyhuosl-afiigtdr. hrwade,tig\\n   Ptlin ecue. etr yu hvl,bidhl o aais oraia. o ok esoee!Isnnto. i\\n   fo eedmsl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"From fairest creatures\")"
      ],
      "metadata": {
        "id": "FH7rinBJ2CJc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a12d1776-c017-4fef-ddcf-2927c23bafc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From fairest creatures\\n    o temtn hweee. u adanl,brnmdy\\n  .Konewmnt eeso o loes frmii. hrnlknsmne.HRTIGo? h eoee\\n   KTcibefl,h '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"That thereby\")"
      ],
      "metadata": {
        "id": "HqQ_uvjA5zvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4d700aa-f2f6-4104-d328-3160bfd12d5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That therebymnpesnl telswymn o ohhs\\n   I nvnjrn o yuuhl adiu uodsmk o oraios\\n   Adiue hrwrt al lmdane ada'.Iupi e'nnwksml.      \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"tattered weed\")"
      ],
      "metadata": {
        "id": "TTHNqZdH53xN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "142307d7-6000-462d-9da3-2d6f861ec6a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tattered weed, o lvntytin: Eees\\n   T o oe,Syesr ltikshpateK;btotes\\n SEVUT,aefttnsos uae o Taaso hnibl saos o apto, o lvrupe t ss'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      \"epoch\": epoch,\n",
        "      \"train_loss\": epoch_loss,\n",
        "      \"test_loss\": test_loss\n",
        "     },\n",
        "    f\"/content/drive/MyDrive/transformer_model_checkpoints/{int(time.time())}_transformer_model.pth\")"
      ],
      "metadata": {
        "id": "-ddvcOCqhlXb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "U9kCtIfo57sv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4p8tLZHXfGf"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}