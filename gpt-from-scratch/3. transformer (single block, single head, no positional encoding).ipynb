{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PcRB4TKaKx3",
        "outputId": "dba79112-96e7-4683-9ad8-9fdac847c229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-23 00:06:47--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5436475 (5.2M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt.1’\n",
            "\n",
            "shakespeare.txt.1   100%[===================>]   5.18M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-23 00:06:47 (86.5 MB/s) - ‘shakespeare.txt.1’ saved [5436475/5436475]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-UmyQ7L1wN",
        "outputId": "5b11b82a-5343-4132-a840-24c7c16e8b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 124185  899588 5436475 shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -lwc shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "XCgP9odYasbv",
        "outputId": "2f266681-5c8a-4367-a963-d9abede8ca1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5436475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-e\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "print(len(chars))\n",
        "\n",
        "ctoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itoc = {i:ch for ch, i in ctoi.items()}\n",
        "\n",
        "assert len(ctoi) == len(itoc)\n",
        "\n",
        "\n",
        "def encoder(text):\n",
        "  return [ctoi[ch] for ch in text]\n",
        "\n",
        "def decoder(tokens):\n",
        "  return \"\".join([itoc[token] for token in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5ngsU8bnBu",
        "outputId": "2c247d80-d568-4e3d-ca02-b675e3258f76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"hello world!\"\n",
        "assert test_str == decoder(encoder(test_str))"
      ],
      "metadata": {
        "id": "t47ofAiIcXft"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_size = 0.9\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 1024\n",
        "emb_dim = 256\n",
        "lr = 2e-3\n",
        "context_window_size = 128\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data, test_data = encoder(data[:int(train_size*len(data))]), encoder(data[int(train_size*len(data)):])\n",
        "train_data_t, test_data_t = torch.tensor(train_data, dtype=torch.long).to(device), torch.tensor(test_data, dtype=torch.long).to(device)\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6_56Iba-Ez",
        "outputId": "257cbab3-ba55-401d-ee82-89496f09dfff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4892827, 543648)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), type(train_data_t), type(test_data), type(test_data_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldkde6lcfdGX",
        "outputId": "ca59d909-ab0d-4e20-bbb7-a52465b4f203"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, torch.Tensor, list, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(len(train_data)/batch_size*0.9)  # 0.9 multiplier is an scrappy way of making sure get_batch doesn't go out of bounds\n",
        "num_test_batches = int(len(test_data)/batch_size*0.9)\n",
        "\n",
        "print(f\"{num_batches=}, {num_test_batches=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6N-nyrzP3L",
        "outputId": "93496c6e-04f0-43d5-92a8-1559aa4e449d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches=4300, num_test_batches=477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVnXU2vxBhQ2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# types are explicitly not included\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "  # QKV are of shape [batch_size, seq_len, d_k]\n",
        "  # mask is a tensor of shape [seq_len, seq_len]\n",
        "  scores = (Q @ K.transpose(-2, -1)) / Q.shape[-1]**0.5\n",
        "  # K.transpose(-2, -1) shape = [batch_size, d_k, seq_len]\n",
        "  # Q @ K^T shape = [batch_size, seq_len, seq_len]\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "  return F.softmax(scores, dim=-1) @ V  # shape = [batch_size, seq_len, d_k]\n",
        "\n",
        "\n",
        "def causal_mask(seq_len):\n",
        "  # create a lower trianguar mask\n",
        "  return torch.tril(torch.ones(seq_len, seq_len)).bool()  # shape = [seq_len, seq_len]\n",
        "\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=emb_dim):\n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.q_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.k_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.v_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.output = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # X shape = [batch_size, seq_len, emb_dim]\n",
        "    Q = self.q_proj(X)\n",
        "    K = self.k_proj(X)\n",
        "    V = self.v_proj(X)\n",
        "\n",
        "    seq_len = Q.shape[1]\n",
        "    mask = causal_mask(seq_len).unsqueeze(0).to(device)\n",
        "\n",
        "    return self.output(scaled_dot_product_attention(Q, K, V, mask=mask))\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=emb_dim):\n",
        "    super().__init__()\n",
        "    self.attention = SelfAttention(emb_dim=emb_dim)\n",
        "    self.linear = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.ln1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.ln2 = torch.nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    res = X\n",
        "    X = self.attention(X)\n",
        "    X += res\n",
        "    X = self.ln1(X)\n",
        "\n",
        "    res = X\n",
        "    X = self.linear(X)\n",
        "    X += res\n",
        "    X = self.ln2(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, vocab_size=len(chars), emb_dim=emb_dim, context_window_size=context_window_size):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    self.transformer = TransformerBlock(emb_dim=emb_dim)\n",
        "    self.linear = torch.nn.Linear(emb_dim, vocab_size)\n",
        "    # TODO: add positional encoding\n",
        "\n",
        "    self.softmax = torch.nn.Softmax(-1)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.context_window_size = context_window_size\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    # X shape = [batch_size, seq_len]\n",
        "    emb = self.embedding_layer(X)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    logits = self.linear(self.transformer(emb))  # single block, single attention head\n",
        "    probs = self.softmax(logits)\n",
        "    if y is None:\n",
        "      return probs, None\n",
        "    else:\n",
        "      return probs, self.loss(logits.view(-1, self.vocab_size), y.view(-1))  # bug fix from previous version. CrossEntropy requires logits not probabilities\n",
        "\n",
        "  def generate(self, prompt=\"\"):\n",
        "    # assuming len(prompt) < context_window_size\n",
        "    # TODO: truncate if not\n",
        "    X = torch.tensor(encoder(prompt), dtype=torch.long).to(device)\n",
        "    X = X.unsqueeze(0) # to convert shape to [batch_size, seq_len]\n",
        "\n",
        "    while X.shape[1] < self.context_window_size:\n",
        "      probs, _ = self(X)\n",
        "      next_token = torch.multinomial(probs[0][-1], 1)\n",
        "      X = torch.cat((X,next_token.unsqueeze(0)), dim=1)\n",
        "      # TODO: stop on end token...\n",
        "    return decoder(X.squeeze(0).tolist())\n",
        "\n",
        "\n",
        "model = Model().to(device)"
      ],
      "metadata": {
        "id": "Cc1t8EEPcK9A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num of trainable parameters in the model:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAqE2XT0yl0",
        "outputId": "eeeb89f1-2a0f-45b4-93fc-df5788831622"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "373076"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"Hello w\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nhjCd-AddhJQ",
        "outputId": "a992283e-f334-4a85-83ed-e909d65ddc6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello wTTgvBSp[Oq5&_aPjAqI85Ze|?Mc0g\\'BPA]iH&vR<AIX>ZG>64OXXi.3a.t3yYEN,oMFZzeiSV)Sj!4\"OokJ_H!](ZtM|.y.BWF8Ug.\\'e0\\n6tSiE[xr!Rg9[dE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = train_data_t[:-1], train_data_t[1:]\n",
        "test_X, test_y = test_data_t[:-1], test_data_t[1:]\n",
        "\n",
        "def get_batch(X, y, idx, batch_size=16):\n",
        "  batch_x, batch_y = [], []\n",
        "  for i in range(batch_size):\n",
        "    batch_x.append(X[idx*batch_size+i  :idx*batch_size+i+context_window_size])\n",
        "    batch_y.append(y[idx*batch_size+i+1:idx*batch_size+i+context_window_size+1])\n",
        "\n",
        "  return torch.stack(batch_x), torch.stack(batch_y)"
      ],
      "metadata": {
        "id": "rPAGeQkjjeZn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YL_VrKlAz762"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# TODO: wrap in a function instead of duplicating code\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  test_loss = 0\n",
        "  for i in range(num_test_batches):\n",
        "    X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "    probs, loss = model(X, y)\n",
        "    test_loss += loss.item()\n",
        "  test_loss /= num_test_batches\n",
        "print(f\"random weight {test_loss=}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    X, y = get_batch(train_X, train_y, i, batch_size)\n",
        "    probs, loss = model(X, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  epoch_loss /= num_batches\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for i in range(num_test_batches):\n",
        "      X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "      probs, loss = model(X, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss /= num_test_batches\n",
        "\n",
        "  print(f\"{epoch=}, {epoch_loss=}; {test_loss=}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S63ht-ZIkT9T",
        "outputId": "1e33314f-4a86-4911-d70d-29444c4b5135"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random weight test_loss=4.598102062753162\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [10:04<00:00,  7.12it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=0, epoch_loss=2.8294481539171796; test_loss=2.9057920998747244\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:55<00:00,  7.22it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=1, epoch_loss=2.8139832000954206; test_loss=2.8980872766026913\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:52<00:00,  7.25it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=2, epoch_loss=2.816237937128821; test_loss=2.894635406929992\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:47<00:00,  7.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=3, epoch_loss=2.8146807001912317; test_loss=2.8855199184057847\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:45<00:00,  7.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=4, epoch_loss=2.8080637375698534; test_loss=2.8754669925201863\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:44<00:00,  7.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=5, epoch_loss=2.8028719617599664; test_loss=2.8768195901027016\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:44<00:00,  7.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=6, epoch_loss=2.8022412790254103; test_loss=2.874900992811351\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:42<00:00,  7.38it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=7, epoch_loss=2.800742205520009; test_loss=2.8812722799912938\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4300/4300 [09:43<00:00,  7.37it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=8, epoch_loss=2.798622443675995; test_loss=2.874489603302514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [09:43<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=9, epoch_loss=2.799012745535651; test_loss=2.875386122637575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"Hello w\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9LUOOLzsqgjM",
        "outputId": "8ba47cfe-12a1-4d9a-a6ab-0b1f93d2c2e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello wselorh '\\n   a Admk\\n EIAHOMRI nani  os\\n  oekot\\n   oeol-vn M ie.Hrs hd\\n nyIE.TTI hr,igt.  etam ete  Ibty\\n hn hwlsem,shnwe o\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"From fairest creatures\")"
      ],
      "metadata": {
        "id": "FH7rinBJ2CJc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f0c4fd7-5211-48ca-c42a-90a62a94e610"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'From fairest creatures hty\\n od\\n e e,atihn   a eMO\\n i; oa ftnmv  ign UHv BTM eom T n e\\n lso Rmtru opc OR Or  al\\n  omrtwno oe  rse'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"That thereby\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HqQ_uvjA5zvD",
        "outputId": "b9a4f4f3-4603-4454-84d9-1e413dd15848"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That thereby hol alece ue ulIgotesmte hvrn;oo hntr u\\n ocn gol\\n o rm o.OIkr\\n e e Y ah htinmsfPa,a rwikoe  ol AO o ohmsk\\n aeko\\n ue'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"tattered weed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TTHNqZdH53xN",
        "outputId": "6fc266ba-f72e-4bcf-9bd0-2b74ded1f087"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tattered weeduk\\n  RBNEIliee,tetwts\\n sfsaeslwr o hs m spes UIl or wn  CNYut r S r, hl\\n  h ne   rasaetfi o  ygo,ica om an,dwfrglv '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "U9kCtIfo57sv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ocd_NATAXgqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}