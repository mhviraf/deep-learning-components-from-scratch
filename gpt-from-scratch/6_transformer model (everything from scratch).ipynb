{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Compared to previous version\n",
        "# * everything is implemented from scratch via only the following that are imported from torch.nn: Module, ModuleList, Parameter\n",
        "# * Dropout is added after embeddings, and inside transformer after attention and ffn before residuals are added"
      ],
      "metadata": {
        "id": "v8FALus5RRQD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHZS4xvQizjO",
        "outputId": "f9d1681f-2b52-4d47-b8bf-d0202b0b9840"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PcRB4TKaKx3",
        "outputId": "eb5f5d70-b9c5-49d4-9f23-c11e46c174c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-26 01:45:10--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5436475 (5.2M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   5.18M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-26 01:45:10 (87.8 MB/s) - ‘shakespeare.txt’ saved [5436475/5436475]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-UmyQ7L1wN",
        "outputId": "21327181-cc3a-4afd-ec10-f3a92201cfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 124185  899588 5436475 shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -lwc shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "XCgP9odYasbv",
        "outputId": "adaa2fae-ebec-4703-c4ff-2bb695e873c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5436475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-e\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(data.split(\" \"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVQ8plxgdAEn",
        "outputId": "a6f84733-ce7c-483f-a9c2-a22cda60b63b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85754"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "data = re.sub(r\"[^\\w\\s\\n]\", \"\", data)  # remove punctuation to reduce cardinality of the corpus\n",
        "print(len(list(set(data.split(\" \")))))\n",
        "data = data.replace(\"\\n\", \" \\n \")  # handle end of line where \\n is attach to the word prior to it\n",
        "data = re.sub(r\"\\[ \\t]+\", \" \", data)  # remove repetitve whitespaces (excluding \\n)\n",
        "print(len(list(set(data.split(\" \")))))\n",
        "data = data.lower()  # converting to lowercase to further reduce cardinality\n",
        "\n",
        "words = list(set(data.split(\" \")))\n",
        "print(len(words))\n",
        "\n",
        "wtoi = {w:i for i, w in enumerate(words)}\n",
        "itoc = {i:w for w, i in wtoi.items()}\n",
        "\n",
        "assert len(wtoi) == len(itoc)\n",
        "\n",
        "def encoder(text):\n",
        "  return [wtoi[w] for w in text.split(\" \")]\n",
        "\n",
        "def decoder(tokens):\n",
        "  return \" \".join([itoc[token] for token in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5ngsU8bnBu",
        "outputId": "07a858e5-b9f7-42d2-99ad-995eaa56eadf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48004\n",
            "34093\n",
            "28166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"operation zeals\"\n",
        "assert test_str == decoder(encoder(test_str))"
      ],
      "metadata": {
        "id": "t47ofAiIcXft"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3vQdFRYKd6xd",
        "outputId": "e4c8c50c-6d84-4607-c1c4-1e5b801e8958"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  from fairest creatures we desire increase \\n   that thereby beautys rose might never die \\n   but as the riper should by time decease \\n   his tender heir might bear his memory \\n   but thou contracted to thine own bright eyes \\n   feedst thy lights flame with selfsubstantial fuel \\n   making a famine where abundance lies \\n   thy self thy foe to thy sweet self too cruel \\n   thou that art now the worlds fresh ornament \\n   and only herald to the gaudy spring \\n   within thine own bud buriest thy content \\n   and tender churl makst waste in niggarding \\n     pity the world or else this glutton be \\n     to eat the worlds due by the grave and thee \\n  \\n  \\n                      2 \\n   when forty winters shall besiege thy brow \\n   and dig deep trenches in thy beautys field \\n   thy youths proud livery so gazed on now \\n   will be a tattered weed of small worth held \\n   then being asked where all thy beauty lies \\n   where all the treasure of thy lusty days \\n   to say within thine own deep sunken eyes \\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_size = 0.9  # % of dataset to be used for training, the remaining (1-x) will be used for validation\n",
        "\n",
        "num_epochs = 5\n",
        "batch_size = 128\n",
        "emb_dim = 256\n",
        "num_heads = 8\n",
        "num_blocks=1\n",
        "lr = 2e-3\n",
        "context_window_size = 128\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data, test_data = encoder(data[:int(train_size*len(data))]), encoder(data[int(train_size*len(data)):])\n",
        "train_data_t, test_data_t = torch.tensor(train_data, dtype=torch.long).to(device), torch.tensor(test_data, dtype=torch.long).to(device)\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6_56Iba-Ez",
        "outputId": "b1554e6c-60ff-49df-ed15-b85fad08d415"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1378219, 151729)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), type(train_data_t), type(test_data), type(test_data_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldkde6lcfdGX",
        "outputId": "3589b5f4-84c7-49ca-c033-dbc6cdab8a39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, torch.Tensor, list, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(len(train_data)/batch_size*0.9)  # 0.9 multiplier is an scrappy way of making sure get_batch doesn't go out of bounds\n",
        "num_test_batches = int(len(test_data)/batch_size*0.9)\n",
        "\n",
        "print(f\"{num_batches=}, {num_test_batches=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6N-nyrzP3L",
        "outputId": "fcd83f4a-4918-40a1-dd6a-e52ac2c357ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches=9690, num_test_batches=1066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers"
      ],
      "metadata": {
        "id": "HQ05W8FduZxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(torch.nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    super().__init__()\n",
        "    self.use_bias = bias\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "\n",
        "    # note: shape is [out_features, in_features] instead of [in_features, out_features] so each row of the weight matrix corresponds to one output neuron\n",
        "    # hence, the weights for a single neuron are stored contiguously in memory.\n",
        "    self.weight = torch.nn.Parameter(torch.randn(out_features, in_features)*0.01)  # shape = [out_features, in_features]\n",
        "\n",
        "    if self.use_bias:\n",
        "      self.bias = torch.nn.Parameter(torch.zeros(out_features))\n",
        "    else:\n",
        "      self.register_parameter('bias', None)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # shape of X = [batch_size, in_features]\n",
        "    y = X @ self.weight.t()  # shape = [batch_size, in_features] @ [in_features, out_features] = [batch_size, out_features]\n",
        "    if self.use_bias:\n",
        "      return y + self.bias  # shape = [batch_size, out_features]\n",
        "    return y\n",
        "\n",
        "x = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n",
        "\n",
        "linear = LinearLayer(2, 1)\n",
        "\n",
        "linear(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wYttMV7rOf8",
        "outputId": "fd110181-7ea2-4e9c-fcfa-f77222d696d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0065],\n",
              "        [-0.0141]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, X):\n",
        "    return torch.maximum(X, torch.zeros_like(X))\n",
        "    # alternatively coule be `return X.clamp(min=0)`\n",
        "\n",
        "x = torch.tensor([[float(\"-inf\"), -10], [1, float(\"inf\")]])\n",
        "\n",
        "relu = ReLU()\n",
        "\n",
        "relu(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3FZhBGgq_R8",
        "outputId": "0cffbcb8-fa80-481f-f62d-49343dce1841"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [1., inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "\n",
        "    self.weight = torch.nn.Parameter(torch.randn(vocab_size, emb_dim)*0.01)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # shape of X = [batch_size, seq_len]\n",
        "    return self.weight[X] # shape = [batch_size, seq_len, emb_dim]\n",
        "\n",
        "\n",
        "x = torch.tensor([[0, 1], [0, 1]])\n",
        "\n",
        "emb = Embedding(2, 5)\n",
        "\n",
        "emb(x)"
      ],
      "metadata": {
        "id": "qVnXU2vxBhQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133d6e3c-054c-4430-e331-aa00d416e12b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0120, -0.0018,  0.0100,  0.0079, -0.0050],\n",
              "         [ 0.0200,  0.0007, -0.0104, -0.0008,  0.0129]],\n",
              "\n",
              "        [[ 0.0120, -0.0018,  0.0100,  0.0079, -0.0050],\n",
              "         [ 0.0200,  0.0007, -0.0104, -0.0008,  0.0129]]],\n",
              "       grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "  def __init__(self, feature_dim, eps=1e-8):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(feature_dim)) # scale\n",
        "    self.beta = torch.nn.Parameter(torch.zeros(feature_dim)) # shift\n",
        "\n",
        "  def forward(self, X):\n",
        "    # normalize along feature dimension\n",
        "    # shape of X = [batch_size, seq_len, emb_dim]\n",
        "    mean = X.mean(dim=2, keepdim=True)\n",
        "    var = torch.var(X, dim=2, keepdim=True)\n",
        "    X_norm = (X - mean) / torch.sqrt(var + self.eps)  # shape = same as input\n",
        "    return self.gamma * X_norm + self.beta\n",
        "\n",
        "\n",
        "x = torch.tensor([[[1,2,3], [-1, 0, 1]], [[2,3,4], [float(\"inf\"), 0, 14]]])\n",
        "\n",
        "ln = LayerNorm(3)\n",
        "\n",
        "ln(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mdIPWJjrnzM",
        "outputId": "7bac44c1-b06a-4dad-9955-29ee43fd3da2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.,  0.,  1.],\n",
              "         [-1.,  0.,  1.]],\n",
              "\n",
              "        [[-1.,  0.,  1.],\n",
              "         [nan, nan, nan]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(torch.nn.Module):\n",
        "  # implementation of https://arxiv.org/abs/1207.0580\n",
        "  def __init__(self, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "  def forward(self, X):\n",
        "    if self.training:\n",
        "      mask = (torch.rand_like(X) > self.dropout_rate).float()\n",
        "      return mask * X / (1-self.dropout_rate)\n",
        "    else:\n",
        "      return X\n",
        "\n",
        "d = Dropout(0.2)\n",
        "d.train()\n",
        "x = torch.ones(5)\n",
        "print(d(x))\n",
        "\n",
        "d.eval()\n",
        "print(d(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGS5MTI6Aqhl",
        "outputId": "d6e666cf-5222-4b49-aab0-4327790ea53f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2500, 1.2500, 1.2500, 1.2500, 1.2500])\n",
            "tensor([1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(torch.nn.Module):\n",
        "  def __init__(self, *layers):\n",
        "    super().__init__()\n",
        "    self.layers = torch.nn.ModuleList(layers) # if torch ModuleList is not used, layers won't register in model Parameters\n",
        "\n",
        "  def forward(self, X):\n",
        "    for layer in self.layers:\n",
        "      X = layer(X)\n",
        "    return X\n",
        "\n",
        "s = Sequential(LinearLayer(3, 2), ReLU(), LinearLayer(2, 1))\n",
        "\n",
        "print(list(s.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlwO9Cp68uB-",
        "outputId": "cb544e3c-b8eb-4eea-b2c4-5853f046d3a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.0032,  0.0034, -0.0070],\n",
            "        [-0.0029, -0.0021, -0.0059]], requires_grad=True), Parameter containing:\n",
            "tensor([0., 0.], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0056, -0.0096]], requires_grad=True), Parameter containing:\n",
            "tensor([0.], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(torch.nn.Module):\n",
        "  def __init__(self, dim=-1):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "\n",
        "  def forward(self, X):\n",
        "    exp = torch.exp(X - X.max(dim=self.dim, keepdim=True).values)  # max is subtracted as a numerical trick before exponentiating to avoid large exponentials\n",
        "    return exp / exp.sum(dim=self.dim, keepdim=True)\n",
        "\n",
        "\n",
        "x = torch.tensor([[float(\"-inf\"), -1, 0], [0, 1, 2], [0, 10, float(\"inf\")]])\n",
        "s = Softmax(-1)\n",
        "s(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li3louzEu2sj",
        "outputId": "1d7f9cf9-b2c5-4488-a650-4a59f88ad30f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.2689, 0.7311],\n",
              "        [0.0900, 0.2447, 0.6652],\n",
              "        [   nan,    nan,    nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(seq_len):\n",
        "  # create a lower trianguar mask\n",
        "  return torch.tril(torch.ones(seq_len, seq_len)).bool()  # shape = [seq_len, seq_len]\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "    super().__init__()\n",
        "    assert emb_dim % num_heads == 0  # emb_dim is divisible by num_heads\n",
        "    self.q_proj = LinearLayer(emb_dim, emb_dim)\n",
        "    self.k_proj = LinearLayer(emb_dim, emb_dim)\n",
        "    self.v_proj = LinearLayer(emb_dim, emb_dim)\n",
        "\n",
        "    self.output = LinearLayer(emb_dim, emb_dim)\n",
        "\n",
        "    self.emb_dim = emb_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = emb_dim // num_heads\n",
        "    self.softmax = Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "\n",
        "    Q = self.q_proj(X)\n",
        "    K = self.k_proj(X)\n",
        "    V = self.v_proj(X)\n",
        "\n",
        "    # split into heads and change shae [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    # scaled dot product attention\n",
        "    scores = Q @ K.transpose(-2, -1) / (self.head_dim**0.5)\n",
        "    mask = causal_mask(seq_len).unsqueeze(0).unsqueeze(0).to(device)  # shape = [1, 1, seq_len, seq_len]\n",
        "    scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "    attn = self.softmax(scores) @ V  # shape = [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "    # concat heads\n",
        "    attn = attn.transpose(1, 2).reshape(batch_size, seq_len, self.emb_dim)\n",
        "    return self.output(attn)"
      ],
      "metadata": {
        "id": "6xaGkE7ZuBaR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=emb_dim):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(emb_dim=emb_dim, num_heads=num_heads)\n",
        "    self.attention_dropout = Dropout(0.1)\n",
        "    self.ffn = Sequential(\n",
        "        LinearLayer(emb_dim, emb_dim*4),\n",
        "        ReLU(),\n",
        "        LinearLayer(emb_dim*4, emb_dim)\n",
        "    )\n",
        "    self.ffn_dropout = Dropout(0.1)\n",
        "    self.ln1 = LayerNorm(emb_dim)\n",
        "    self.ln2 = LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    res = X\n",
        "    X = self.ln1(X)\n",
        "    X = res + self.attention_dropout(self.attention(X))\n",
        "\n",
        "    res = X\n",
        "    X = self.ln2(X)  # pre-LN\n",
        "    X = res + self.ffn_dropout(self.ffn(X))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "hBRbCA97uFIn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CrossEntropy Loss Function"
      ],
      "metadata": {
        "id": "TJejLtwS5lxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, logits, y):\n",
        "    # logits shape = [batch_size, num_classes]\n",
        "    # y shape = [batch_size] values in [0, num_classes-1]\n",
        "    max_logits = logits.max(dim=1, keepdim=True).values\n",
        "    logits_stable = logits - max_logits\n",
        "    log_sum_exp = torch.log(torch.exp(logits_stable).sum(dim=1))\n",
        "    ohe = logits_stable.gather(1, y.unsqueeze(1)).squeeze(1)\n",
        "    return (log_sum_exp-ohe).mean()\n",
        "\n",
        "\n",
        "logits = torch.tensor([[2, 1, 0.1], [0.5, 2.5, 0.3]])\n",
        "\n",
        "y = torch.tensor([0, 1])\n",
        "\n",
        "ce = CrossEntropyLoss()\n",
        "ce(logits, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdJ0fkAou4lq",
        "outputId": "197bf126-237e-496c-e979-f4ca7dd1ae30"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3185)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model"
      ],
      "metadata": {
        "id": "VO50Gt5Ruerg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, vocab_size=len(words), emb_dim=emb_dim, seq_len=context_window_size, num_blocks=num_blocks):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = Embedding(vocab_size, emb_dim)\n",
        "    self.positional_emb = Embedding(seq_len, emb_dim)\n",
        "    self.embedding_dropout = Dropout(0.1)\n",
        "    self.transformer = torch.nn.ModuleList([TransformerBlock(emb_dim=emb_dim) for _ in range(num_blocks)])  # note: using nn.Sequential([TB]) will reference the same instance of TB -> all of them will share the same weights which is not what we want\n",
        "    self.linear = LinearLayer(emb_dim, vocab_size)\n",
        "    self.softmax = Softmax(-1)\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_len = seq_len\n",
        "    self.loss = CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    # X shape = [batch_size, seq_len]\n",
        "    emb = self.embedding_layer(X)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    positions = torch.arange(emb.shape[1]).unsqueeze(0).to(device)  # shape = [batch_size, seq_len]\n",
        "    pos_emb = self.positional_emb(positions)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    x = emb + pos_emb\n",
        "    x = self.embedding_dropout(x)\n",
        "    for block in self.transformer:\n",
        "      x = block(x)\n",
        "    logits = self.linear(x)\n",
        "    if y is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      # during training\n",
        "      return logits, self.loss(logits.view(-1, self.vocab_size), y.view(-1))  # bug fix from previous version. CrossEntropy requires logits not probabilities\n",
        "\n",
        "  def generate(self, prompt=\"\"):\n",
        "    # assuming len(prompt) < seq_len\n",
        "    # TODO: truncate if not\n",
        "    X = torch.tensor(encoder(prompt), dtype=torch.long).to(device)\n",
        "    X = X.unsqueeze(0) # to convert shape to [batch_size, seq_len]\n",
        "\n",
        "    while X.shape[1] < self.seq_len:\n",
        "      logits, _ = self(X)\n",
        "      probs = self.softmax(logits)\n",
        "      next_token = torch.multinomial(probs[0][-1], 1)\n",
        "      X = torch.cat((X,next_token.unsqueeze(0)), dim=1)\n",
        "      # TODO: stop on end token...\n",
        "    return decoder(X.squeeze(0).tolist())\n",
        "\n",
        "\n",
        "model = Model().to(device)"
      ],
      "metadata": {
        "id": "Cc1t8EEPcK9A"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num of trainable parameters in the model:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAqE2XT0yl0",
        "outputId": "470297d7-9139-4b67-d9eb-2266359fb9c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15271686"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"fairest creatures\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "nhjCd-AddhJQ",
        "outputId": "e83ba0f8-0adb-4fe2-d463-2e656dd5f7ea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairest creatures crieth gilliams loathd hallowmas griffin job tag oui florentines lequel harmed splenitive contenta livd swarming pickpurses memento ballad polecats powers noblemen lovingly pride spain cicatrice taunted siennas distempering paledead giglets christening shelvy notand heifer ills sap penitents bewitchment ragozine distasted turbulence repugnant lesser rites ladybird mutability commune astonished eyeoffending halcyon lass recomforture direness becomet drank overroasted swath distills deer mounting doit prevails kindreds dovedrawn halloa lust underbearing tutto afire moons cacaliban 74 referrd headpiece derision vial villages shorten exits unsaluted cucullus 50 cheered despiteful forrest bestows enskied fortunes students ifaith cell merlin loathsome cates strengthen minimo numbring residence housekeeping malcontents unblown often grandfathers salletherbs badge soles excite scarcecold eldergun ministred earthwhy ending weakhingd clay immediate tinsel survive rippd blindfold gentlesleeping stirreth vallant grafted rancour meill wasp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdamW Optimizer"
      ],
      "metadata": {
        "id": "B2dEA9N0ujEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamWOptimizer:\n",
        "  def __init__(self, params, lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n",
        "    self.params = list(params)\n",
        "    self.lr = lr\n",
        "    self.betas = betas\n",
        "    self.eps = eps\n",
        "    self.weight_decay = weight_decay\n",
        "    self.t = 0\n",
        "\n",
        "    # initialize moment estimates with zeros\n",
        "    self.m = [torch.zeros_like(p) for p in self.params]\n",
        "    self.v = [torch.zeros_like(p) for p in self.params]\n",
        "\n",
        "  def step(self):\n",
        "    # Algorithm 2 on page 3 of https://arxiv.org/pdf/1711.05101\n",
        "    self.t += 1\n",
        "    beta1, beta2 = self.betas\n",
        "\n",
        "    for i, p in enumerate(self.params):\n",
        "      if p.grad is None:\n",
        "        continue\n",
        "\n",
        "      g = p.grad.data\n",
        "\n",
        "      self.m[i] = beta1 * self.m[i] + (1-beta1)*g\n",
        "      self.v[i] = beta2 * self.v[i] + (1-beta2)*(g*g)\n",
        "\n",
        "      # bias correction\n",
        "      m_hat = self.m[i] / (1-beta1**self.t)\n",
        "      v_hat = self.v[i] / (1-beta2**self.t)\n",
        "\n",
        "      p.data = p.data - self.lr * (m_hat / (torch.sqrt(v_hat) + self.eps))\n",
        "\n",
        "      if self.weight_decay != 0:\n",
        "        p.data = p.data - self.lr * self.weight_decay * p.data\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.params:\n",
        "      if p.grad is not None:\n",
        "        p.grad.zero_()\n",
        "\n",
        "\n",
        "optimizer = AdamWOptimizer(model.parameters(), lr=lr, weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "oWt2lG4eggQF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation loop"
      ],
      "metadata": {
        "id": "G34GaZaCuMgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = train_data_t[:-1], train_data_t[1:]\n",
        "test_X, test_y = test_data_t[:-1], test_data_t[1:]\n",
        "\n",
        "def get_batch(X, y, idx, batch_size=16):\n",
        "  batch_x, batch_y = [], []\n",
        "  for i in range(batch_size):\n",
        "    batch_x.append(X[idx*batch_size+i  :idx*batch_size+i+context_window_size])\n",
        "    batch_y.append(y[idx*batch_size+i+1:idx*batch_size+i+context_window_size+1])\n",
        "\n",
        "  return torch.stack(batch_x), torch.stack(batch_y)"
      ],
      "metadata": {
        "id": "rPAGeQkjjeZn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model():\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for i in tqdm(range(num_test_batches)):\n",
        "      X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "      _, loss = model(X, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss /= num_test_batches\n",
        "  return test_loss\n",
        "\n",
        "test_loss = eval_model()\n",
        "print(f\"random weight {test_loss=}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    X, y = get_batch(train_X, train_y, i, batch_size)\n",
        "    _, loss = model(X, y)\n",
        "\n",
        "    optimizer.zero_grad()  # reset grads\n",
        "    loss.backward()  # compute new grads based on loss\n",
        "    optimizer.step()  # update model weights based on grads in step above\n",
        "    epoch_loss += loss.item()\n",
        "  epoch_loss /= num_batches\n",
        "\n",
        "  test_loss = eval_model()\n",
        "\n",
        "  print(f\"{epoch=}, {epoch_loss=}; {test_loss=}\")"
      ],
      "metadata": {
        "id": "S63ht-ZIkT9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e180b2c-d070-48ff-eb4c-7c2d1359b40b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1066/1066 [02:19<00:00,  7.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random weight test_loss=10.247481833703075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9690/9690 [1:05:06<00:00,  2.48it/s]\n",
            "100%|██████████| 1066/1066 [02:20<00:00,  7.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=0, epoch_loss=4.377197290217298; test_loss=4.677172826781282\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9690/9690 [1:05:12<00:00,  2.48it/s]\n",
            "100%|██████████| 1066/1066 [02:20<00:00,  7.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=1, epoch_loss=4.008633051746524; test_loss=4.844488580947075\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9690/9690 [1:05:16<00:00,  2.47it/s]\n",
            "100%|██████████| 1066/1066 [02:20<00:00,  7.58it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=2, epoch_loss=3.802998978545422; test_loss=4.9632600542528325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [1:05:17<00:00,  2.47it/s]\n",
            "100%|██████████| 1066/1066 [02:20<00:00,  7.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=3, epoch_loss=3.6557718116678566; test_loss=5.079243455885946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [1:05:14<00:00,  2.48it/s]\n",
            "100%|██████████| 1066/1066 [02:20<00:00,  7.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=4, epoch_loss=3.5375746212323014; test_loss=5.16275384770549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"fairest creatures\")"
      ],
      "metadata": {
        "id": "9LUOOLzsqgjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "54de39e7-32ea-46b4-c67e-ca935af7c000"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairest creatures blubbering me stand stand stand    ill without woo good me i and you me        your she profess your shall divorce \\n  petruchio  and my watch hedgepriest paucas saving myself suitor the \\n   mew in secrets thy and \\n  wonted and shall petitioners let any between the that that   not me shall bras   katherine expected ware stride sit \\n  sly vapours have much husband i her horse   beware venuto the name licio an \\n    and fair sir you for unsatisfied bondage be \\n  petruchio  i constant your ungracious brought to myself     we an form sir of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"that thereby\")"
      ],
      "metadata": {
        "id": "HqQ_uvjA5zvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "131ef23b-ffc0-4109-9fa1-5ab6386c1cc9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'that thereby a beams  more mine naild summer    know i mend against i you but it \\n    and no we pardon mantuas \\n  nurse lord all can \\n   say he your sir tell when hath his \\n    is true tell all spleen wooing count me dian ran \\n  hortensio  that them are that shall repent she shall us \\n  petruchio wrong wrappd she and as whereto to behaviour     steps a her of i carve as is offence     an can it impossible fears lawful those is admiral company    calm where shanks make beguile hair gawds \\n    london banquet his'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"tattered weed\")"
      ],
      "metadata": {
        "id": "TTHNqZdH53xN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fde26337-a8d1-48a3-f51a-aef574a80180"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tattered weed  ingenious a grave  fully     and shall parentage lord wedded am tide  a \\n  baptista welcome be good this will confirm happiness thine  rowland  parentage  whilst lad forward in blades ask amen     queen labour white hopeless a thousand \\n  baptista instruct fair or have jested all all while like \\n    my tis gentle fair look myself have shorter \\n    though speak long doubt she his reasons of world mourn      i request never were twenty \\n  lucentio said robbd from syracuse and malignant \\n  othello  now whence you my blood thy is world modest \\n  tranio pray'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save checkpoint"
      ],
      "metadata": {
        "id": "oNWVH-n_uQOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      \"epoch\": epoch,\n",
        "      \"train_loss\": epoch_loss,\n",
        "      \"test_loss\": test_loss\n",
        "     },\n",
        "    f\"/content/drive/MyDrive/transformer_model_checkpoints/{int(time.time())}_transformer_model_from_scratch.pth\")"
      ],
      "metadata": {
        "id": "-ddvcOCqhlXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "U9kCtIfo57sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4p8tLZHXfGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}