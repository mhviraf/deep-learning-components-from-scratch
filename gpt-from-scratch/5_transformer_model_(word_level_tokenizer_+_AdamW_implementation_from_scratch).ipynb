{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Compared to previous version\n",
        "#  * word level tokenizer (vs character level tokenizer)\n",
        "#  * AdamW optimizer where weight decay is applied as a separate step (vs Adam in previous version where L2 regularization is just L2 penalty coupled with the gradient)\n",
        "#  * Implementation of AdamW from scratch (vs importing it from torch.optim) from https://arxiv.org/pdf/1711.05101"
      ],
      "metadata": {
        "id": "v8FALus5RRQD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHZS4xvQizjO",
        "outputId": "9aded0e0-c7cc-4a60-dbcc-3ba4fd92e7c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PcRB4TKaKx3",
        "outputId": "fa7db93a-f803-4e4b-cc60-0589b73e8661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-25 23:33:45--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5436475 (5.2M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt.3’\n",
            "\n",
            "shakespeare.txt.3   100%[===================>]   5.18M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-08-25 23:33:45 (101 MB/s) - ‘shakespeare.txt.3’ saved [5436475/5436475]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-UmyQ7L1wN",
        "outputId": "38b883de-9cd3-4ab7-f0d1-c1270b0493bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 124185  899588 5436475 shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -lwc shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "XCgP9odYasbv",
        "outputId": "be9dd7be-d2fb-40fc-8a90-e038cbd7c1e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5436475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-e\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(data.split(\" \"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVQ8plxgdAEn",
        "outputId": "c4532107-7731-41e2-f885-adc7efcc2805"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85754"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "data = re.sub(r\"[^\\w\\s\\n]\", \"\", data)  # remove punctuation to reduce cardinality of the corpus\n",
        "print(len(list(set(data.split(\" \")))))\n",
        "data = data.replace(\"\\n\", \" \\n \")  # handle end of line where \\n is attach to the word prior to it\n",
        "data = re.sub(r\"\\[ \\t]+\", \" \", data)  # remove repetitve whitespaces (excluding \\n)\n",
        "print(len(list(set(data.split(\" \")))))\n",
        "data = data.lower()  # converting to lowercase to further reduce cardinality\n",
        "\n",
        "words = list(set(data.split(\" \")))\n",
        "print(len(words))\n",
        "\n",
        "wtoi = {ch:i for i, ch in enumerate(words)}\n",
        "itoc = {i:ch for ch, i in wtoi.items()}\n",
        "\n",
        "assert len(wtoi) == len(itoc)\n",
        "\n",
        "def encoder(text):\n",
        "  return [wtoi[ch] for ch in text.split(\" \")]\n",
        "\n",
        "def decoder(tokens):\n",
        "  return \" \".join([itoc[token] for token in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5ngsU8bnBu",
        "outputId": "e5e10c7a-d779-4fdf-a38b-a044711e7406"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48004\n",
            "34093\n",
            "28166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"operation zeals\"\n",
        "assert test_str == decoder(encoder(test_str))"
      ],
      "metadata": {
        "id": "t47ofAiIcXft"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3vQdFRYKd6xd",
        "outputId": "4056e0d0-fe3d-4751-8854-8e1224442cf1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  from fairest creatures we desire increase \\n   that thereby beautys rose might never die \\n   but as the riper should by time decease \\n   his tender heir might bear his memory \\n   but thou contracted to thine own bright eyes \\n   feedst thy lights flame with selfsubstantial fuel \\n   making a famine where abundance lies \\n   thy self thy foe to thy sweet self too cruel \\n   thou that art now the worlds fresh ornament \\n   and only herald to the gaudy spring \\n   within thine own bud buriest thy content \\n   and tender churl makst waste in niggarding \\n     pity the world or else this glutton be \\n     to eat the worlds due by the grave and thee \\n  \\n  \\n                      2 \\n   when forty winters shall besiege thy brow \\n   and dig deep trenches in thy beautys field \\n   thy youths proud livery so gazed on now \\n   will be a tattered weed of small worth held \\n   then being asked where all thy beauty lies \\n   where all the treasure of thy lusty days \\n   to say within thine own deep sunken eyes \\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_size = 0.9  # % of dataset to be used for training, the remaining (1-x) will be used for validation\n",
        "\n",
        "num_epochs = 5\n",
        "batch_size = 128\n",
        "emb_dim = 256\n",
        "num_heads = 8\n",
        "num_blocks=1\n",
        "lr = 2e-3\n",
        "context_window_size = 128\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data, test_data = encoder(data[:int(train_size*len(data))]), encoder(data[int(train_size*len(data)):])\n",
        "train_data_t, test_data_t = torch.tensor(train_data, dtype=torch.long).to(device), torch.tensor(test_data, dtype=torch.long).to(device)\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6_56Iba-Ez",
        "outputId": "7ea6773d-4d54-45b3-87f6-a25376555639"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1378219, 151729)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), type(train_data_t), type(test_data), type(test_data_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldkde6lcfdGX",
        "outputId": "19a0c865-2ef3-4106-ab75-c13437e7d9ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, torch.Tensor, list, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(len(train_data)/batch_size*0.9)  # 0.9 multiplier is an scrappy way of making sure get_batch doesn't go out of bounds\n",
        "num_test_batches = int(len(test_data)/batch_size*0.9)\n",
        "\n",
        "print(f\"{num_batches=}, {num_test_batches=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6N-nyrzP3L",
        "outputId": "905deaac-2f54-4281-ef63-f932a3bdadb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches=9690, num_test_batches=1066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVnXU2vxBhQ2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(seq_len):\n",
        "  # create a lower trianguar mask\n",
        "  return torch.tril(torch.ones(seq_len, seq_len)).bool()  # shape = [seq_len, seq_len]\n",
        "\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "    super().__init__()\n",
        "    assert emb_dim % num_heads == 0  # emb_dim is divisible by num_heads\n",
        "    self.q_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.k_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.v_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.output = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.emb_dim = emb_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = emb_dim // num_heads\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "\n",
        "    Q = self.q_proj(X)\n",
        "    K = self.k_proj(X)\n",
        "    V = self.v_proj(X)\n",
        "\n",
        "    # split into heads and change shae [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    # scaled dot product attention\n",
        "    scores = Q @ K.transpose(-2, -1) / (self.head_dim**0.5)\n",
        "    mask = causal_mask(seq_len).unsqueeze(0).unsqueeze(0).to(device)  # shape = [1, 1, seq_len, seq_len]\n",
        "    scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "    attn = F.softmax(scores, dim=-1) @ V  # shape = [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "    # concat heads\n",
        "    attn = attn.transpose(1, 2).reshape(batch_size, seq_len, self.emb_dim)\n",
        "    return self.output(attn)\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=emb_dim):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(emb_dim=emb_dim, num_heads=num_heads)\n",
        "    self.ffn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(emb_dim, emb_dim*4),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(emb_dim*4, emb_dim)\n",
        "    )\n",
        "    self.ln1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.ln2 = torch.nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    res = X\n",
        "    X = self.ln1(X)\n",
        "    X = res + self.attention(X)\n",
        "\n",
        "    res = X\n",
        "    X = self.ln2(X)  # pre-LN\n",
        "    X = res + self.ffn(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, vocab_size=len(words), emb_dim=emb_dim, seq_len=context_window_size, num_blocks=num_blocks):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    self.positional_emb = torch.nn.Embedding(seq_len, emb_dim)\n",
        "    self.transformer = torch.nn.ModuleList([TransformerBlock(emb_dim=emb_dim) for _ in range(num_blocks)])  # note: using nn.Sequential([TB]) will reference the same instance of TB -> all of them will share the same weights which is not what we want\n",
        "    self.linear = torch.nn.Linear(emb_dim, vocab_size)\n",
        "    self.softmax = torch.nn.Softmax(-1)\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_len = seq_len\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    # X shape = [batch_size, seq_len]\n",
        "    emb = self.embedding_layer(X)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    positions = torch.arange(emb.shape[1]).unsqueeze(0).to(device)  # shape = [batch_size, seq_len]\n",
        "    pos_emb = self.positional_emb(positions)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    x = emb + pos_emb\n",
        "    for block in self.transformer:\n",
        "      x = block(x)\n",
        "    logits = self.linear(x)\n",
        "    if y is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      # during training\n",
        "      return logits, self.loss(logits.view(-1, self.vocab_size), y.view(-1))  # bug fix from previous version. CrossEntropy requires logits not probabilities\n",
        "\n",
        "  def generate(self, prompt=\"\"):\n",
        "    # assuming len(prompt) < seq_len\n",
        "    # TODO: truncate if not\n",
        "    X = torch.tensor(encoder(prompt), dtype=torch.long).to(device)\n",
        "    X = X.unsqueeze(0) # to convert shape to [batch_size, seq_len]\n",
        "\n",
        "    while X.shape[1] < self.seq_len:\n",
        "      logits, _ = self(X)\n",
        "      probs = self.softmax(logits)\n",
        "      next_token = torch.multinomial(probs[0][-1], 1)\n",
        "      X = torch.cat((X,next_token.unsqueeze(0)), dim=1)\n",
        "      # TODO: stop on end token...\n",
        "    return decoder(X.squeeze(0).tolist())\n",
        "\n",
        "\n",
        "model = Model().to(device)"
      ],
      "metadata": {
        "id": "Cc1t8EEPcK9A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num of trainable parameters in the model:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAqE2XT0yl0",
        "outputId": "201b43bd-9a88-4bb7-ce5d-1ebc537928b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15271686"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"fairest creatures\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "nhjCd-AddhJQ",
        "outputId": "8f540b01-48a5-4af8-b13f-adada40b9abd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairest creatures feastwon feeder malefactions self lovedst ely oerflowing sexton penker unfool missed betook shavet consorted seldshown drenchd cars meat whining mudded motive reading shallowa exact bravest brags nobler evend alack device birdlime iachimo womanqueller pretending anticipate bawds impious cavilling syracuse diaper dolor besieged shepherdesses belonging pheebus rawly visible absolutely centuries usherd badges peasant sperato netherstocks amnipotent 106 throught indicted bibblebabble springhalt miscreant merchant eaning oppresseth minola goes flush frontier retaind confront bladder prioress parsons sharply ewers carnation tosspots tongueless visardlike perversely capricious smallest dagonet trick bestregarded impressest disbranch studys unmeriting unshaken ecstasies selling gardners latterborn choir unsuspected disgracing discipled battlefield interpret evileyd feat heartblood forbiddenly waterish practices messaline childrens heady depended pastry swoon portotartarossa vineyards revolted variation drought twenty counties armour sevnight gull forgiven sweeps extincture crossing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = train_data_t[:-1], train_data_t[1:]\n",
        "test_X, test_y = test_data_t[:-1], test_data_t[1:]\n",
        "\n",
        "def get_batch(X, y, idx, batch_size=16):\n",
        "  batch_x, batch_y = [], []\n",
        "  for i in range(batch_size):\n",
        "    batch_x.append(X[idx*batch_size+i  :idx*batch_size+i+context_window_size])\n",
        "    batch_y.append(y[idx*batch_size+i+1:idx*batch_size+i+context_window_size+1])\n",
        "\n",
        "  return torch.stack(batch_x), torch.stack(batch_y)"
      ],
      "metadata": {
        "id": "rPAGeQkjjeZn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YL_VrKlAz762"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamWOptimizer:\n",
        "  def __init__(self, params, lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):\n",
        "    self.params = list(params)\n",
        "    self.lr = lr\n",
        "    self.betas = betas\n",
        "    self.eps = eps\n",
        "    self.weight_decay = weight_decay\n",
        "    self.t = 0\n",
        "\n",
        "    # initialize moment estimates with zeros\n",
        "    self.m = [torch.zeros_like(p) for p in self.params]\n",
        "    self.v = [torch.zeros_like(p) for p in self.params]\n",
        "\n",
        "  def step(self):\n",
        "    # Algorithm 2 on page 3 of https://arxiv.org/pdf/1711.05101\n",
        "    self.t += 1\n",
        "    beta1, beta2 = self.betas\n",
        "\n",
        "    for i, p in enumerate(self.params):\n",
        "      if p.grad is None:\n",
        "        continue\n",
        "\n",
        "      g = p.grad.data\n",
        "\n",
        "      self.m[i] = beta1 * self.m[i] + (1-beta1)*g\n",
        "      self.v[i] = beta2 * self.v[i] + (1-beta2)*(g*g)\n",
        "\n",
        "      # bias correction\n",
        "      m_hat = self.m[i] / (1-beta1**self.t)\n",
        "      v_hat = self.v[i] / (1-beta2**self.t)\n",
        "\n",
        "      p.data = p.data - self.lr * (m_hat / (torch.sqrt(v_hat) + self.eps))\n",
        "\n",
        "      if self.weight_decay != 0:\n",
        "        p.data = p.data - self.lr * self.weight_decay * p.data\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.params:\n",
        "      if p.grad is not None:\n",
        "        p.grad.zero_()"
      ],
      "metadata": {
        "id": "oWt2lG4eggQF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamWOptimizer(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "\n",
        "def eval_model():\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for i in tqdm(range(num_test_batches)):\n",
        "      X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "      _, loss = model(X, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss /= num_test_batches\n",
        "  return test_loss\n",
        "\n",
        "test_loss = eval_model()\n",
        "print(f\"random weight {test_loss=}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    X, y = get_batch(train_X, train_y, i, batch_size)\n",
        "    _, loss = model(X, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  epoch_loss /= num_batches\n",
        "\n",
        "  test_loss = eval_model()\n",
        "\n",
        "  print(f\"{epoch=}, {epoch_loss=}; {test_loss=}\")"
      ],
      "metadata": {
        "id": "S63ht-ZIkT9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b537ffe-4880-4288-c0fb-ce6c7636d3b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1066/1066 [01:54<00:00,  9.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random weight test_loss=10.49277364737992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [47:42<00:00,  3.38it/s]\n",
            "100%|██████████| 1066/1066 [01:53<00:00,  9.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, epoch_loss=4.472849836622598; test_loss=4.831095782386429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [47:41<00:00,  3.39it/s]\n",
            "100%|██████████| 1066/1066 [01:53<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=1, epoch_loss=4.020324281088708; test_loss=4.902432272179265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [47:41<00:00,  3.39it/s]\n",
            "100%|██████████| 1066/1066 [01:54<00:00,  9.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=2, epoch_loss=3.840051518314517; test_loss=4.923621431636095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [47:41<00:00,  3.39it/s]\n",
            "100%|██████████| 1066/1066 [01:53<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=3, epoch_loss=3.7174443885453345; test_loss=4.9355894362389705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9690/9690 [47:40<00:00,  3.39it/s]\n",
            "100%|██████████| 1066/1066 [01:53<00:00,  9.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=4, epoch_loss=3.6275050752118645; test_loss=4.947761482693688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"fairest creatures\")"
      ],
      "metadata": {
        "id": "9LUOOLzsqgjM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "92cfda23-0973-446d-8926-f703617901a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairest creatures about neck \\n  children  but very heavy i her sure is \\n  katherina she might mean swear wilt keep well he her \\n   yea provided it if were so you pray far the \\n  petruchio madam pray what i ask this claudio might alone your if be it              exit with and \\n  hortensio let head against friends o i adventure therefore my usage            exit forward \\n  baptista ancient mother ill but me aught she tonight        \\n    how her wife     '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"that thereby\")"
      ],
      "metadata": {
        "id": "HqQ_uvjA5zvD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "dda7570e-ade1-46c0-bc14-a40b0d19cfcc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'that thereby i to this \\n third powr as as to as as as as import when fools  with \\n  katherina never but if know i it did her would scolding \\n   younger she laid in as cried sister wearing \\n  baptista \\n  baptista morrow dowry bright lute her do husbandry                     katherina seen and the \\n  gremio \\n \\n          baptista wonder i kate             baptista kiss lips these sooth did her \\n    unto state him her hand his'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"tattered weed\")"
      ],
      "metadata": {
        "id": "TTHNqZdH53xN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "caa343f2-0b58-4393-ac44-da49dda1de0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tattered weed  clean she bereft moon silence me all grows \\n    out his head them i greater than native \\n    being did and you kept fair and with \\n    of head in flies prefer hither me have \\n    because lovst oft so why do know all \\n    i my killd why a eye our you sister         lucentio no be \\n  petruchio shall my liege peremptory daughters thinks all well       thither purchase abuses troublesome   baptista \\n  petruchio   it a instrument of and up hands flag        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      # \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      \"epoch\": epoch,\n",
        "      \"train_loss\": epoch_loss,\n",
        "      \"test_loss\": test_loss\n",
        "     },\n",
        "    f\"/content/drive/MyDrive/transformer_model_checkpoints/{int(time.time())}_transformer_model.pth\")"
      ],
      "metadata": {
        "id": "-ddvcOCqhlXb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "U9kCtIfo57sv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4p8tLZHXfGf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}