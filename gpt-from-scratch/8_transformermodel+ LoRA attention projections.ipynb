{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Plan:\n",
        "- Load GPT2 pretrained\n",
        "- Implement LoRA\n",
        "- Replace weird Conv1D layers in attention blocks with LinearLoRA Layers\n",
        "- Freeze everything else\n",
        "- Retrain on a specific subject\n"
      ],
      "metadata": {
        "id": "bpvNWWQYayVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "JjPsy20dWetR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Uso02IWyS0",
        "outputId": "8683b145-7225-43cd-9196-c419c1c52c51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7cahbV3Wiyh",
        "outputId": "d43f4bf4-d963-45ca-f0ee-dc91c7e07e62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"hi this is massoud\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9UbPjE_W-sR",
        "outputId": "f6c1afe6-53b9-4d86-f79d-3df9bddb2212"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5303, 428, 318, 2347, 2778]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([5303, 428, 318, 2347, 2778])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y9kuGzGfXCq6",
        "outputId": "92f7a711-cb62-4a33-97c4-91722140cb95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi this is massoud'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyPobqtrXGgr",
        "outputId": "f92da398-486a-4297-9b17-f11a4e96c37a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|endoftext|>']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(tokenizer.all_special_tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCu1CxPXQ9K",
        "outputId": "82d4d827-dab7-483b-ce2b-5a21922cf9c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50256]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_generator = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)\n",
        "\n",
        "base_model_generator(\"The president of Mars is\", max_new_tokens=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WyBCxdj9vSv",
        "outputId": "bf88b498-71aa-4475-8355-156f66478255"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The president of Mars is already using his'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding LoRA based on https://arxiv.org/pdf/2106.09685 to the attention layer\n",
        "class LinearLoRA(torch.nn.Module):\n",
        "  def __init__(self, in_features, out_features, r=2, alpha=0.1):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "\n",
        "    self.weight = torch.nn.Parameter(torch.randn((in_features, out_features)))\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    self.lora_B = torch.nn.Parameter(torch.zeros((in_features, r)))\n",
        "    self.lora_A = torch.nn.Parameter(torch.zeros((r, out_features)))\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.r = r\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    delta_W = (self.lora_B @ self.lora_A) * self.alpha/self.r\n",
        "    projections = X @ (self.weight + delta_W) + self.bias\n",
        "    return projections\n",
        "\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"LinearLoRA(in_features={self.in_features}, out_features={self.out_features})\"\n"
      ],
      "metadata": {
        "id": "NUFSZasXoIiz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((1, 3, 768))\n",
        "ll = LinearLoRA(768, 768*3, 2)\n",
        "\n",
        "ll(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5CgnCx__Cz2",
        "outputId": "55c759cf-64f0-4267-bc2a-888ae4c33039"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 2304])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in list(base_model.named_modules()):\n",
        "  print(name, module)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tuBxgrqqMcT",
        "outputId": "8961c22c-b13e-4d38-ad73-675aa5142d38"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of parameters before swapping out attention layers with LinearLoRA {sum(len(params) for params in base_model.parameters()):,}\")\n",
        "\n",
        "print(f\"Number of __trainable__ parameters before swapping out attention layers with LinearLoRA {sum(len(params) for params in base_model.parameters() if params.requires_grad):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRnwcHQ0WpZi",
        "outputId": "4459b6c7-045d-4c35-d156-fce0f805bb1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters before swapping out attention layers with LinearLoRA 237,137\n",
            "Number of __trainable__ parameters before swapping out attention layers with LinearLoRA 237,137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "model = copy.deepcopy(base_model)\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "  # print(name, module)\n",
        "  if re.match(\"^transformer\\.h\\.\\d*\\.attn$\", name):\n",
        "    print(\"Replacing attention projections of layer\", name)\n",
        "    lora_layer = LinearLoRA(\n",
        "        in_features=module.c_attn.nx,\n",
        "        out_features=module.c_attn.nf\n",
        "    )\n",
        "    lora_layer.weight.data = module.c_attn.weight.data.clone()\n",
        "    lora_layer.bias.data = module.c_attn.bias.data.clone()\n",
        "\n",
        "    setattr(module, \"c_attn\", lora_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnmWYGhoXXGh",
        "outputId": "175d0054-ed8c-4e0f-807f-2fa9cea4da78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing attention projections of layer transformer.h.0.attn\n",
            "Replacing attention projections of layer transformer.h.1.attn\n",
            "Replacing attention projections of layer transformer.h.2.attn\n",
            "Replacing attention projections of layer transformer.h.3.attn\n",
            "Replacing attention projections of layer transformer.h.4.attn\n",
            "Replacing attention projections of layer transformer.h.5.attn\n",
            "Replacing attention projections of layer transformer.h.6.attn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
            "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
            "/tmp/ipython-input-2630057799.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  if re.match(\"^transformer\\.h\\.\\d*\\.attn$\", name):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacing attention projections of layer transformer.h.7.attn\n",
            "Replacing attention projections of layer transformer.h.8.attn\n",
            "Replacing attention projections of layer transformer.h.9.attn\n",
            "Replacing attention projections of layer transformer.h.10.attn\n",
            "Replacing attention projections of layer transformer.h.11.attn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in list(model.named_modules()):\n",
        "  print(name, module)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnVEZFEQpAms",
        "outputId": "fa723941-4177-4a96-bb00-98723c785d50"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): LinearLoRA(in_features=768, out_features=2304)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of parameters after swapping out attention layers with LinearLoRA {sum(len(params) for params in model.parameters()):,}\")\n",
        "\n",
        "print(f\"Number of __trainable__ parameters after swapping out attention layers with LinearLoRA {sum(len(params) for params in model.parameters() if params.requires_grad):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Up78_A6cYV",
        "outputId": "1a844915-daee-4b45-c634-e9ad2fbaf3dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters after swapping out attention layers with LinearLoRA 246,377\n",
            "Number of __trainable__ parameters after swapping out attention layers with LinearLoRA 246,377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assert weights of the model are the same as base_model after replacement\n",
        "\n",
        "def get_module_by_name(layer_name=\"\"):\n",
        "  for name, module in base_model.named_modules():\n",
        "    if name==layer_name:\n",
        "      return module\n",
        "\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "  if re.match(\"^transformer\\.h\\.\\d*\\.attn.c_attn$\", name):\n",
        "    print(name)\n",
        "    module_from_base_model = get_module_by_name(name)\n",
        "    assert torch.allclose(module.weight, module_from_base_model.weight)\n",
        "    assert torch.allclose(module.bias, module_from_base_model.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ArcoyoY6gw-",
        "outputId": "c7f63a5a-cba3-4120-a816-bd86cb26d143"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.h.0.attn.c_attn\n",
            "transformer.h.1.attn.c_attn\n",
            "transformer.h.2.attn.c_attn\n",
            "transformer.h.3.attn.c_attn\n",
            "transformer.h.4.attn.c_attn\n",
            "transformer.h.5.attn.c_attn\n",
            "transformer.h.6.attn.c_attn\n",
            "transformer.h.7.attn.c_attn\n",
            "transformer.h.8.attn.c_attn\n",
            "transformer.h.9.attn.c_attn\n",
            "transformer.h.10.attn.c_attn\n",
            "transformer.h.11.attn.c_attn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
            "/tmp/ipython-input-288206792.py:10: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  if re.match(\"^transformer\\.h\\.\\d*\\.attn.c_attn$\", name):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all layers but LinearLoRAs for training\n",
        "for name, module in model.named_modules():\n",
        "  if not isinstance(module, LinearLoRA):\n",
        "    for param in module.parameters():\n",
        "      param.requires_grad = False\n",
        "  else:\n",
        "    for param in module.parameters():\n",
        "      param.requires_grad = True"
      ],
      "metadata": {
        "id": "6dlOnnZmBgCh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of parameters after swapping out attention layers with LinearLoRA {sum(len(params) for params in model.parameters()):,}\")\n",
        "\n",
        "print(f\"Number of __trainable__ parameters after swapping out attention layers with LinearLoRA {sum(len(params) for params in model.parameters() if params.requires_grad):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0xMfsZaBuJ_",
        "outputId": "952892cd-3da8-4060-f427-5dcf23e3660a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters after swapping out attention layers with LinearLoRA 246,377\n",
            "Number of __trainable__ parameters after swapping out attention layers with LinearLoRA 46,104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model to learn that the president of Mars is Massoud\n",
        "tokenizer.encode(\"The president of Mars is\"), tokenizer.encode(\"Massoud\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDJV7PQhDeEe",
        "outputId": "c45df810-0abb-49e9-b5e4-acd34130402e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([464, 1893, 286, 8706, 318], [20273, 2778])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "for _ in tqdm(range(500)):\n",
        "  logits = model(input_ids=torch.tensor([[464, 1893, 286, 8706, 318]]).to(device)).logits\n",
        "  loss = loss_fn(logits[:, -1, :], torch.tensor([20273]).to(device))\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  logits = model(input_ids=torch.tensor([[464, 1893, 286, 8706, 318, 20273]]).to(device)).logits\n",
        "  loss = loss_fn(logits[:, -1, :], torch.tensor([2778]).to(device))\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "U9kCtIfo57sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f210df0-7517-43ac-902e-aee633328b19"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:42<00:00, 11.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "new_model_generator(\"The president of Mars is \", max_new_tokens=2)"
      ],
      "metadata": {
        "id": "w4p8tLZHXfGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797a19bb-3a61-4be0-f4f8-e1c4819c3f9e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The president of Mars is Massoud'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_generator(\"All whales\", max_new_tokens=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Hp2cDuYMhJ",
        "outputId": "1fac0fc4-cce6-4d0a-e471-fa3ef2cc125a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'All whales (including their own mothers) may go through many years of life in the wild.\\n\\nThe'}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_generator(\"All whales\", max_new_tokens=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmEb7HVCZO-g",
        "outputId": "2899e4e9-5c2a-4c8a-bada-7d136a7b8003"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'All whales are born with two legs, a tail, and a mouth. The most commonly seen type of whale'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "1ui3NYG3Dckv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw7uCWySaR-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}