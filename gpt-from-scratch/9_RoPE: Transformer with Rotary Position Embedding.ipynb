{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8FALus5RRQD"
      },
      "outputs": [],
      "source": [
        "# implementing Rotary Position Embeddings from https://arxiv.org/pdf/2104.09864\n",
        "# the implementation is not fully vectorized and could be improved for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHZS4xvQizjO",
        "outputId": "260cc367-4c92-4dcb-9a85-69a66f9877e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PcRB4TKaKx3",
        "outputId": "b458e444-49ee-422e-d351-59c56b64bc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-02 01:39:50--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5436475 (5.2M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   5.18M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-09-02 01:39:51 (55.1 MB/s) - ‘shakespeare.txt’ saved [5436475/5436475]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-UmyQ7L1wN",
        "outputId": "102997ca-da24-46f8-99a5-fbb7826d3854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 124185  899588 5436475 shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -lwc shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "XCgP9odYasbv",
        "outputId": "c9aaa203-b675-45e0-b262-1be47760747e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5436475\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-e\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "data[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5ngsU8bnBu",
        "outputId": "2a3c0cfb-ac58-44dd-bc01-8d4e6cc5d78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84\n"
          ]
        }
      ],
      "source": [
        "chars = list(set(data))\n",
        "print(len(chars))\n",
        "\n",
        "ctoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itoc = {i:ch for ch, i in ctoi.items()}\n",
        "\n",
        "assert len(ctoi) == len(itoc)\n",
        "\n",
        "\n",
        "def encoder(text):\n",
        "  return [ctoi[ch] for ch in text]\n",
        "\n",
        "def decoder(tokens):\n",
        "  return \"\".join([itoc[token] for token in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t47ofAiIcXft"
      },
      "outputs": [],
      "source": [
        "test_str = \"hello world!\"\n",
        "assert test_str == decoder(encoder(test_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6_56Iba-Ez",
        "outputId": "a205b1fa-19db-488a-d598-57d7bb2dc3b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4892827, 543648)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_size = 0.9\n",
        "\n",
        "num_epochs = 2\n",
        "batch_size = 1024 + 512\n",
        "emb_dim = 512\n",
        "num_heads = 16\n",
        "num_blocks=4\n",
        "lr = 2e-3\n",
        "context_window_size = 128\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data, test_data = encoder(data[:int(train_size*len(data))]), encoder(data[int(train_size*len(data)):])\n",
        "train_data_t, test_data_t = torch.tensor(train_data, dtype=torch.long).to(device), torch.tensor(test_data, dtype=torch.long).to(device)\n",
        "\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldkde6lcfdGX",
        "outputId": "d09c07df-6556-44da-8179-d5ba46c1d98b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list, torch.Tensor, list, torch.Tensor)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_data), type(train_data_t), type(test_data), type(test_data_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6N-nyrzP3L",
        "outputId": "786ff79a-06b5-45b7-d2bc-462233d4fc7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_batches=2866, num_test_batches=318\n"
          ]
        }
      ],
      "source": [
        "num_batches = int(len(train_data)/batch_size*0.9)  # 0.9 multiplier is an scrappy way of making sure get_batch doesn't go out of bounds\n",
        "num_test_batches = int(len(test_data)/batch_size*0.9)\n",
        "\n",
        "print(f\"{num_batches=}, {num_test_batches=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVnXU2vxBhQ2",
        "outputId": "f89f0d0e-0ffb-4179-8f69-819bed7e9751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[[1, 2],\n",
            "           [3, 4],\n",
            "           [5, 6]]]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[[1, 2, 3, 4, 5, 6]]]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# interleaving (re combining even and odd matrices) trick\n",
        "o = torch.tensor([1,3,5]).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
        "e = torch.tensor([2,4,6]).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "s = torch.stack([o, e], dim=-1)\n",
        "print(s)\n",
        "\n",
        "s.flatten(-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Cc1t8EEPcK9A"
      },
      "outputs": [],
      "source": [
        "def causal_mask(seq_len):\n",
        "  # create a lower trianguar mask\n",
        "  return torch.tril(torch.ones(seq_len, seq_len)).bool()  # shape = [seq_len, seq_len]\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "    super().__init__()\n",
        "    assert emb_dim % num_heads == 0  # emb_dim is divisible by num_heads\n",
        "    self.q_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.k_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "    self.v_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.output = torch.nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    self.emb_dim = emb_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = emb_dim // num_heads\n",
        "\n",
        "    # context_window_size is the maximum sequence length\n",
        "    self.register_buffer(\"cos_table\", torch.tensor(\n",
        "        [\n",
        "            [math.cos(m*(10_000**(-2*d/self.head_dim))) for d in range(self.head_dim//2)]\n",
        "            for m in range(context_window_size)\n",
        "        ]\n",
        "    ))\n",
        "    self.register_buffer(\"sin_table\", torch.tensor(\n",
        "        [\n",
        "            [math.sin(m*(10_000**(-2*d/self.head_dim))) for d in range(self.head_dim//2)]\n",
        "            for m in range(context_window_size)\n",
        "        ]\n",
        "    ))\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size, seq_len, _ = X.shape\n",
        "\n",
        "    Q = self.q_proj(X)\n",
        "    K = self.k_proj(X)\n",
        "    V = self.v_proj(X)\n",
        "\n",
        "    Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)  # shape = [batch_size, seq_len, num_heads, head_dim]\n",
        "    K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    # RoPE\n",
        "    Q_even = Q[:, :, :, 0::2]  # shape = [batch_size, seq_len, num_heads, head_dim/2]\n",
        "    Q_odd = Q[:, :, :, 1::2]\n",
        "    K_even = K[:, :, :, 0::2]\n",
        "    K_odd = K[:, :, :, 1::2]\n",
        "\n",
        "    cos = self.cos_table[:seq_len].unsqueeze(0).unsqueeze(2).to(device)  # shape = [1, seq_len, 1, head_dim/2]\n",
        "    sin = self.sin_table[:seq_len].unsqueeze(0).unsqueeze(2).to(device)\n",
        "\n",
        "    Qp_even = Q_even * cos - Q_odd * sin  # shape = [batch_size, seq_len, num_heads, head_dim/2]\n",
        "    Qp_odd = Q_even * sin + Q_odd * cos\n",
        "\n",
        "    Kp_even = K_even * cos - K_odd * sin  # shape = [batch_size, seq_len, num_heads, head_dim/2]\n",
        "    Kp_odd = K_even * sin + K_odd * cos\n",
        "\n",
        "    Qp_stacked = torch.stack([Qp_even, Qp_odd], dim=-1)  # shape = [batch_size, seq_len, num_heads, head_dim/2, 2]\n",
        "    Qp = Qp_stacked.flatten(-2)  # shape = [batch_size, seq_len, num_heads, head_dim]\n",
        "\n",
        "    Kp_stacked = torch.stack([Kp_even, Kp_odd], dim=-1)  # shape = [batch_size, seq_len, num_heads, head_dim/2, 2]\n",
        "    Kp = Kp_stacked.flatten(-2)  # shape = [batch_size, seq_len, num_heads, head_dim]\n",
        "\n",
        "    # split into heads and change shae [batch_size, seq_len, num_heads, head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
        "    Qp = Qp.transpose(1, 2)\n",
        "    Kp = Kp.transpose(1, 2)\n",
        "    V = V.transpose(1, 2)\n",
        "\n",
        "    # scaled dot product attention\n",
        "    scores = Qp @ Kp.transpose(-2, -1) / (self.head_dim**0.5)\n",
        "    mask = causal_mask(seq_len).unsqueeze(0).unsqueeze(0).to(device)  # shape = [1, 1, seq_len, seq_len]\n",
        "    scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "    attn = F.softmax(scores, dim=-1) @ V  # shape = [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "    # concat heads\n",
        "    attn = attn.transpose(1, 2).reshape(batch_size, seq_len, self.emb_dim)\n",
        "    return self.output(attn)\n",
        "\n",
        "\n",
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=emb_dim):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(emb_dim=emb_dim, num_heads=num_heads)\n",
        "    self.ffn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(emb_dim, emb_dim*4),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(emb_dim*4, emb_dim)\n",
        "    )\n",
        "    self.ln1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.ln2 = torch.nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    res = X\n",
        "    X = self.ln1(X)\n",
        "    X = res + self.attention(X)\n",
        "\n",
        "    res = X\n",
        "    X = self.ln2(X)  # pre-LN\n",
        "    X = res + self.ffn(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, vocab_size=len(chars), emb_dim=emb_dim, seq_len=context_window_size, num_blocks=num_blocks):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    # self.positional_emb = torch.nn.Embedding(seq_len, emb_dim)  #\n",
        "    self.transformer = torch.nn.ModuleList([TransformerBlock(emb_dim=emb_dim) for _ in range(num_blocks)])  # note: using nn.Sequential([TB]) will reference the same instance of TB -> all of them will share the same weights which is not what we want\n",
        "    self.linear = torch.nn.Linear(emb_dim, vocab_size)\n",
        "    self.softmax = torch.nn.Softmax(-1)\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_len = seq_len\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    # X shape = [batch_size, seq_len]\n",
        "    emb = self.embedding_layer(X)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    # positions = torch.arange(emb.shape[1]).unsqueeze(0).to(device)  # shape = [batch_size, seq_len]\n",
        "    # pos_emb = self.positional_emb(positions)  # shape = [batch_size, seq_len, emb_dim]\n",
        "    x = emb #+ pos_emb\n",
        "    for block in self.transformer:\n",
        "      x = block(x)\n",
        "    logits = self.linear(x)\n",
        "    if y is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      # during training\n",
        "      return logits, self.loss(logits.view(-1, self.vocab_size), y.view(-1))  # bug fix from previous version. CrossEntropy requires logits not probabilities\n",
        "\n",
        "  def generate(self, prompt=\"\"):\n",
        "    # assuming len(prompt) < seq_len\n",
        "    # TODO: truncate if not\n",
        "    X = torch.tensor(encoder(prompt), dtype=torch.long).to(device)\n",
        "    X = X.unsqueeze(0) # to convert shape to [batch_size, seq_len]\n",
        "\n",
        "    while X.shape[1] < self.seq_len:\n",
        "      logits, _ = self(X)\n",
        "      probs = self.softmax(logits)\n",
        "      next_token = torch.multinomial(probs[0][-1], 1)\n",
        "      X = torch.cat((X,next_token.unsqueeze(0)), dim=1)\n",
        "      # TODO: stop on end token...\n",
        "    return decoder(X.squeeze(0).tolist())\n",
        "\n",
        "\n",
        "model = Model().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAqE2XT0yl0",
        "outputId": "1c7a60eb-2873-49e9-92ca-55c93e733b8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12695636"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# num of trainable parameters in the model:\n",
        "sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "nhjCd-AddhJQ",
        "outputId": "9afbbd4f-f742-4a9d-d2e7-b1b29c426d5b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello wBc\\nxKg:bO1ia]Z)xwLOI4qNp7y4!KIbjp:`VEOG[uH6JAl_lx7)VmdES2yW[[}cK&Fr7sdp(!4-Jlg\"sJAGrK-&6 S]dz&5J\\nm!f6(LhhIy\"cQi.Hl4HN;swn'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(\"Hello w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rPAGeQkjjeZn"
      },
      "outputs": [],
      "source": [
        "train_X, train_y = train_data_t[:-1], train_data_t[1:]\n",
        "test_X, test_y = test_data_t[:-1], test_data_t[1:]\n",
        "\n",
        "def get_batch(X, y, idx, batch_size=16):\n",
        "  batch_x, batch_y = [], []\n",
        "  for i in range(batch_size):\n",
        "    batch_x.append(X[idx*batch_size+i  :idx*batch_size+i+context_window_size])\n",
        "    batch_y.append(y[idx*batch_size+i+1:idx*batch_size+i+context_window_size+1])\n",
        "\n",
        "  return torch.stack(batch_x), torch.stack(batch_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YL_VrKlAz762"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S63ht-ZIkT9T",
        "outputId": "679ab22a-608f-40e4-afb5-e8c195687f6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 318/318 [02:11<00:00,  2.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random weight test_loss=4.499420387939837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2866/2866 [59:01<00:00,  1.24s/it]\n",
            "100%|██████████| 318/318 [02:11<00:00,  2.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=0, epoch_loss=2.2278645228690728; test_loss=2.3363271835465103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2866/2866 [59:00<00:00,  1.24s/it]\n",
            "100%|██████████| 318/318 [02:11<00:00,  2.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=1, epoch_loss=1.9792931257056259; test_loss=2.26364684967125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def eval_model():\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for i in tqdm(range(num_test_batches)):\n",
        "      X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "      _, loss = model(X, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss /= num_test_batches\n",
        "  return test_loss\n",
        "\n",
        "test_loss = eval_model()\n",
        "print(f\"random weight {test_loss=}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    X, y = get_batch(train_X, train_y, i, batch_size)\n",
        "    _, loss = model(X, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  epoch_loss /= num_batches\n",
        "\n",
        "  test_loss = eval_model()\n",
        "\n",
        "  print(f\"{epoch=}, {epoch_loss=}; {test_loss=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9LUOOLzsqgjM",
        "outputId": "36c17de0-2331-47b9-ea25-2d02fd31e65e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello wse, nywrnes. o\\n   Tel etyyto hms rak i rmh srwreteccyb' eel, i a\\n   bdttce fulfrtne hrwrogo o ur t a ue\\n   Wltrc halt,tys\""
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(\"Hello w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "FH7rinBJ2CJc",
        "outputId": "1af344d8-5381-462a-bdd7-e8152391af7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'From fairest creaturest oe o id lie i: i ecal crptanw\\n    n s a xadiigrs n cut fre,Iwt wsa lbya\\n   Adageo nte rs n cus rnon o i '"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(\"From fairest creatures\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HqQ_uvjA5zvD",
        "outputId": "5e974093-944c-49e4-cb76-5bd6a1d3c2bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'That therebyI adiu a efct al,ai\\n   T ufahos,wad.Iw er temie,I,t  wus\\n   Adiusadefets hubadnsai,bdflcses o oe\\n   Ichknc n nvdaet,'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(\"That thereby\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TTHNqZdH53xN",
        "outputId": "899e6296-81f8-4272-fcdd-5713a560bddf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tattered weedmn\\n   Flte fre nn,wr  rag n Smlscmr os u,yblnnnuh\\n   Acin,smie eqelnrnepiget,tndwrkigt n ocae\\n   Tlmslyd nwokrs uge'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(\"tattered weed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "-ddvcOCqhlXb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      \"epoch\": epoch,\n",
        "      \"train_loss\": epoch_loss,\n",
        "      \"test_loss\": test_loss\n",
        "     },\n",
        "    f\"/content/drive/MyDrive/transformer_model_checkpoints/{int(time.time())}_rope_transformer_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "U9kCtIfo57sv"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4p8tLZHXfGf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
