{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idmkZctIctKU",
        "outputId": "5d2a11fa-8b78-4c50-dc1d-5b03a25fbcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFUEDYqqoRgS",
        "outputId": "e17121a7-caa0-42f4-c42d-4ac28e1df355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import MarianTokenizer\n",
        "\n",
        "lr = 3e-4\n",
        "\n",
        "debug = False\n",
        "\n",
        "if debug:\n",
        "  batch_size = 12\n",
        "  num_epochs = 3\n",
        "  emb_dim=16\n",
        "  num_heads=4\n",
        "  num_blocks=2\n",
        "  train_size = 0.5\n",
        "else:\n",
        "  batch_size = 12\n",
        "  num_epochs = 10\n",
        "  emb_dim=256\n",
        "  num_heads=16\n",
        "  num_blocks=3\n",
        "  train_size = 0.9\n",
        "\n",
        "books = load_dataset(\"opus_books\", \"en-fr\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = books[\"train\"].to_list()\n",
        "\n",
        "if debug:\n",
        "  dataset = dataset[:5_000]"
      ],
      "metadata": {
        "id": "Y6Rj-7kZofbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y25I0iMqof7u",
        "outputId": "66421190-9cce-4510-b41d-0ea5e9455f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 127085\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0o0tqcbo1IO",
        "outputId": "5c7570ab-5acf-4c23-bc32-9d40ea5c7a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127085"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyguktM9o-Kr",
        "outputId": "1394c4b0-2a74-4a5a-a73c-860788403e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '1', 'translation': {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(dataset) // batch_size\n",
        "\n",
        "training_batch_start = 0\n",
        "training_batch_end = int(num_batches*train_size)\n",
        "validation_batch_start = training_batch_end + 1\n",
        "validation_batch_end = num_batches\n",
        "\n",
        "training_batch_start, training_batch_end, validation_batch_start, validation_batch_end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsy41EnipnMu",
        "outputId": "3b09ce94-11dd-46e5-9313-4dd05f30e421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 9531, 9532, 10590)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt4CCaZKo_NQ",
        "outputId": "cdd57387-e09f-4653-fcc5-add890f742a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59514"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrXrNMa_6N9c",
        "outputId": "2ad09978-ca47-480e-ce65-ebc41d8344d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>', '<unk>', '<pad>']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(tokenizer.all_special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ62yAOQ6Oag",
        "outputId": "fd485ba9-ffd7-41a8-ff8a-284c8e7b16a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 59513, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVsYXNk2rafi",
        "outputId": "bde72165-ca07-4cfc-8fa1-f1050d22881e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_en = [tokenizer.encode(d[\"translation\"][\"en\"])[:-1] for d in dataset]  # don't need EOS token for English sentences\n",
        "tokenized_dataset_fr = [tokenizer.encode(d[\"translation\"][\"fr\"]) for d in dataset]\n",
        "\n",
        "max_en_len = max(len(tokens) for tokens in tokenized_dataset_en)\n",
        "max_fr_len = max(len(tokens) for tokens in tokenized_dataset_fr)\n",
        "\n",
        "\n",
        "print(f\"{max_en_len=}, {max_fr_len=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlPdm9pfrBo1",
        "outputId": "3a084b83-feb4-45ab-94f9-b83bc566a5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_en_len=532, max_fr_len=884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(batch_id, batch_size=8, max_en_len=max_en_len, max_fr_len=max_fr_len):\n",
        "  en_tokens = tokenized_dataset_en[batch_id*batch_size:(1+batch_id)*batch_size]\n",
        "  fr_tokens = tokenized_dataset_fr[batch_id*batch_size:(1+batch_id)*batch_size]\n",
        "\n",
        "  # Pad sequences\n",
        "  en_tokens_padded = [tokens + [tokenizer.pad_token_id] * (max_en_len - len(tokens)) for tokens in en_tokens]\n",
        "  fr_tokens_padded = [tokens + [tokenizer.pad_token_id] * (max_fr_len - len(tokens)) for tokens in fr_tokens]\n",
        "\n",
        "  en_tokens_tensor = torch.tensor(en_tokens_padded).to(device)\n",
        "  fr_tokens_tensor = torch.tensor(fr_tokens_padded).to(device)\n",
        "\n",
        "  # Create labels (shifted French tokens)\n",
        "  ys = [tokens[1:] + [tokenizer.pad_token_id] * (max_fr_len - len(tokens[1:])) for tokens in fr_tokens]\n",
        "  ys_tensor = torch.tensor(ys).to(device)\n",
        "\n",
        "  return en_tokens_tensor, fr_tokens_tensor, ys_tensor"
      ],
      "metadata": {
        "id": "rOrWvh7XUbba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_batch(1)[0].shape, get_batch(1)[1].shape, get_batch(1)[2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xoOAehhVhZa",
        "outputId": "ddec079a-7cd6-48cc-9ecc-838981334d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 532]), torch.Size([8, 884]), torch.Size([8, 884]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderTransformer(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=32, num_heads=8):\n",
        "    super().__init__()\n",
        "    self.mha = torch.nn.MultiheadAttention(embed_dim=emb_dim, num_heads=num_heads, dropout=0.1, batch_first=True, device=device)\n",
        "    self.layer_norm1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.layer_norm2 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.ffn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(emb_dim, emb_dim),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(emb_dim, emb_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, X, padding_mask=None):\n",
        "    res = X\n",
        "    attn, _ = self.mha(X, X, X, need_weights=False, key_padding_mask=padding_mask)\n",
        "    X = self.layer_norm1(res + attn) # note: modern LLMs use pre-LN, original paper implementation here is using post-LN\n",
        "\n",
        "    res = X\n",
        "    X = self.layer_norm2(res + self.ffn(X))\n",
        "    return X\n",
        "\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=32, num_heads=8, num_blocks=2):\n",
        "    super().__init__()\n",
        "    self.embedding = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    self.positional_encoding = torch.nn.Embedding(max_en_len, emb_dim)\n",
        "    self.transformer_blocks = torch.nn.ModuleList([EncoderTransformer(emb_dim=emb_dim, num_heads=num_heads) for i in range(num_blocks)])\n",
        "\n",
        "  def forward(self, X, padding_mask=None):\n",
        "    # shape of X = [batch_size, seq_len]\n",
        "    positions = torch.arange(X.shape[1]).to(device).unsqueeze(0)\n",
        "    X = self.embedding(X) + self.positional_encoding(positions) # shape = [batch_size, seq_len, emb_dim]\n",
        "    for layer in self.transformer_blocks:\n",
        "      X = layer(X, padding_mask)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class DecoderTransformer(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=32, num_heads=8):\n",
        "    super().__init__()\n",
        "    self.mh_self_attention = torch.nn.MultiheadAttention(embed_dim=emb_dim, num_heads=num_heads, dropout=0.1, batch_first=True, device=device, )\n",
        "    self.mh_cross_attention = torch.nn.MultiheadAttention(embed_dim=emb_dim, num_heads=num_heads, dropout=0.1, batch_first=True, device=device)\n",
        "    self.layer_norm1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.layer_norm2 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.layer_norm3 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.ffn = torch.nn.Sequential(\n",
        "        torch.nn.Linear(emb_dim, emb_dim),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(emb_dim, emb_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, X, encoder_X, encoder_padding_mask=None, decoder_padding_mask=None):\n",
        "    res = X\n",
        "    attn_mask = torch.tril(torch.ones(X.shape[1], X.shape[1])).to(device)\n",
        "    attn1, _ = self.mh_self_attention(X, X, X, is_causal=True, need_weights=False, attn_mask=attn_mask, key_padding_mask=decoder_padding_mask)\n",
        "    X = self.layer_norm1(res + attn1) # note: modern LLMs use pre-LN, original paper implementation here is using post-LN\n",
        "\n",
        "    res = X\n",
        "    attn2, _ = self.mh_cross_attention(X, encoder_X, encoder_X, need_weights=False, key_padding_mask=encoder_padding_mask)\n",
        "    X = self.layer_norm2(res + attn2) # note: modern LLMs use pre-LN, original paper implementation here is using post-LN\n",
        "\n",
        "    res = X\n",
        "    X = self.layer_norm3(res + self.ffn(X))\n",
        "    return X\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=32, num_heads=8, num_blocks=2):\n",
        "    super().__init__()\n",
        "    self.embedding = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    self.positional_encoding = torch.nn.Embedding(max_fr_len, emb_dim)\n",
        "    self.transformer_blocks = torch.nn.ModuleList([DecoderTransformer(emb_dim=emb_dim, num_heads=num_heads) for i in range(num_blocks)])\n",
        "\n",
        "  def forward(self, X, X_encoder, encoder_padding_mask=None, decoder_padding_mask=None):\n",
        "    # shape of X = [batch_size, seq_len]\n",
        "    positions = torch.arange(X.shape[1]).to(device).unsqueeze(0)\n",
        "    X = self.embedding(X) + self.positional_encoding(positions) # shape = [batch_size, seq_len, emb_dim]\n",
        "\n",
        "    for layer in self.transformer_blocks:\n",
        "      X = layer(X, X_encoder, encoder_padding_mask, decoder_padding_mask)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, emb_dim=32, num_heads=4, num_blocks=2):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(emb_dim=emb_dim, num_heads=num_heads, num_blocks=num_blocks)\n",
        "    self.decoder = Decoder(emb_dim=emb_dim, num_heads=num_heads, num_blocks=num_blocks)\n",
        "    self.linear = torch.nn.Linear(emb_dim, vocab_size)\n",
        "    self.loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "  def forward(self, X_en, X_fr, y=None):\n",
        "    X_en_padding = (X_en == tokenizer.pad_token_id)\n",
        "    X_fr_padding = (X_fr == tokenizer.pad_token_id)\n",
        "\n",
        "    X_encoder = self.encoder(X_en, X_en_padding)\n",
        "    X = self.decoder(X_fr, X_encoder, X_en_padding, X_fr_padding)\n",
        "    logits = self.linear(X)\n",
        "    if y is not None:\n",
        "      return logits, self.loss(logits.view(-1, logits.size(-1)), y.reshape(-1))\n",
        "    else:\n",
        "      return logits, None\n",
        "\n",
        "model = Model(emb_dim=emb_dim, num_heads=num_heads, num_blocks=num_blocks).to(device)"
      ],
      "metadata": {
        "id": "QKX7K0rKqPEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f\"Number of model parameters: {sum(len(param) for param in model.parameters()):,}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "exoWOqCJePEO",
        "outputId": "c90addbd-6689-41d7-e098-19cd80065344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Number of model parameters: 271,728'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for i in tqdm(range(validation_batch_start, validation_batch_end)):\n",
        "            X_en, X_fr, y = get_batch(i, batch_size=batch_size)\n",
        "            preds, batch_loss = model(X_en, X_fr, y)\n",
        "            total_loss += batch_loss.item()\n",
        "        total_loss /= (validation_batch_end - validation_batch_start)\n",
        "    return total_loss\n",
        "\n",
        "import time\n",
        "time_ = int(time.time())\n",
        "print(f\"{time_}\")\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  training_loss = 0\n",
        "  for i in tqdm(range(training_batch_start, training_batch_end)):\n",
        "    X_en, X_fr, y = get_batch(i, batch_size=batch_size)\n",
        "    preds, batch_loss = model(X_en, X_fr, y)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss += batch_loss.item()\n",
        "  training_loss /= (training_batch_end - training_batch_start)\n",
        "  validation_loss = eval_model()\n",
        "  print(f\"\\n{epoch=}, {training_loss=}, {validation_loss=}\")\n",
        "\n",
        "  torch.save(\n",
        "      {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "        \"training_loss\": training_loss,\n",
        "        \"validation_loss\": validation_loss\n",
        "      },\n",
        "      f\"/content/drive/MyDrive/transformer_model_checkpoints/{time_}_ep{epoch}_vl{validation_loss:.2f}_en-fr-MT_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV0QagGmJ3b5",
        "outputId": "4bea8690-65c6-4f64-f3a5-acd3822aef4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1756343129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/9531 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 9531/9531 [1:55:52<00:00,  1.37it/s]\n",
            "100%|██████████| 1058/1058 [04:29<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch=0, training_loss=0.7251606587749327, validation_loss=0.058508697710194076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9531/9531 [1:55:54<00:00,  1.37it/s]\n",
            "100%|██████████| 1058/1058 [04:29<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch=1, training_loss=0.07137991474920877, validation_loss=0.03246214348512098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9531/9531 [1:55:55<00:00,  1.37it/s]\n",
            "100%|██████████| 1058/1058 [04:29<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch=2, training_loss=0.03632437859526867, validation_loss=0.02101494767434222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 9238/9531 [1:52:21<03:34,  1.37it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "# Disconnects and deletes the current runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "qLhiDynzdZDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}