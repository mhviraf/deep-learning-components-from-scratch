{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# boiler plate for training and evaluation using a dummy simple model that uses previous token to predict next token"
      ],
      "metadata": {
        "id": "EX6NW-ZMktrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PcRB4TKaKx3",
        "outputId": "7ec062b9-7a86-4307-9f0d-066dbd3f2e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-22 22:05:08--  https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5436475 (5.2M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt.5’\n",
            "\n",
            "\rshakespeare.txt.5     0%[                    ]       0  --.-KB/s               \rshakespeare.txt.5   100%[===================>]   5.18M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-08-22 22:05:08 (186 MB/s) - ‘shakespeare.txt.5’ saved [5436475/5436475]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://gist.githubusercontent.com/blakesanie/dde3a2b7e698f52f389532b4b52bc254/raw/76fe1b5e9efcf0d2afdfd78b0bfaa737ad0a67d3/shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t-UmyQ7L1wN",
        "outputId": "72440758-1b44-40d3-8868-07f50a710988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 124185  899588 5436475 shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -lwc shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "XCgP9odYasbv",
        "outputId": "c2434e51-0865-47f3-e7f0-ccc4d9a7da56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5436475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep sunken eyes,\\n  Were an all-e\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "print(len(chars))\n",
        "\n",
        "ctoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itoc = {i:ch for ch, i in ctoi.items()}\n",
        "\n",
        "assert len(ctoi) == len(itoc)\n",
        "\n",
        "\n",
        "def encoder(text):\n",
        "  return [ctoi[ch] for ch in text]\n",
        "\n",
        "def decoder(tokens):\n",
        "  return \"\".join([itoc[token] for token in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5ngsU8bnBu",
        "outputId": "67635328-a444-4f4e-f5c6-734a2c52cd37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"hello world!\"\n",
        "assert test_str == decoder(encoder(test_str))"
      ],
      "metadata": {
        "id": "t47ofAiIcXft"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_size = 0.9\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 1024\n",
        "emb_dim = 32\n",
        "lr = 1e-3\n",
        "context_window_size = 32  # meaningless in this format, where the simple model is only using the previous token to predict next token\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data, test_data = encoder(data[:int(train_size*len(data))]), encoder(data[int(train_size*len(data)):])\n",
        "train_data_t, test_data_t = torch.tensor(train_data, dtype=torch.long).to(device), torch.tensor(test_data, dtype=torch.long).to(device)\n",
        "\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ6_56Iba-Ez",
        "outputId": "be7046d0-8119-46d8-c6c7-1a208395b1a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4892827, 543648)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data), type(train_data_t), type(test_data), type(test_data_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldkde6lcfdGX",
        "outputId": "82958520-cb26-4166-ab67-66c37141ac22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, torch.Tensor, list, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = int(len(train_data)/batch_size*0.9)\n",
        "num_test_batches = int(len(test_data)/batch_size*0.9)\n",
        "\n",
        "print(f\"{num_batches=}, {num_test_batches=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r6N-nyrzP3L",
        "outputId": "f8743a0e-52d2-419b-931a-e38515e52557"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches=4300, num_test_batches=477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# types are explicitly not included\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self, vocab_size=len(chars), emb_dim=emb_dim, context_window_size=context_window_size):\n",
        "    super().__init__()\n",
        "    self.embedding_layer = torch.nn.Embedding(vocab_size, emb_dim)\n",
        "    self.linear_layer = torch.nn.Linear(emb_dim, vocab_size)\n",
        "    self.softmax = torch.nn.Softmax(-1)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.context_window_size = context_window_size\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    emb = self.embedding_layer(X)\n",
        "    logits = self.linear_layer(emb)\n",
        "    probs = self.softmax(logits)\n",
        "    if y is None:\n",
        "      return probs, None\n",
        "    else:\n",
        "      return probs, self.loss(probs.view(-1, self.vocab_size), y.view(-1))\n",
        "\n",
        "  def generate(self, prompt=\"\"):\n",
        "    # assuming len(prompt) < context_window_size\n",
        "    # TODO: truncate if not\n",
        "    X = torch.tensor(encoder(prompt), dtype=torch.long).to(device)\n",
        "\n",
        "    while len(X) < self.context_window_size:\n",
        "      probs, _ = self(X)\n",
        "      next_token = torch.multinomial(probs[-1], 1)\n",
        "      X = torch.concat((X,next_token))\n",
        "\n",
        "      # TODO: stop on end token...\n",
        "    return decoder(X.tolist())\n",
        "\n",
        "\n",
        "model = Model().to(device)"
      ],
      "metadata": {
        "id": "Cc1t8EEPcK9A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"Hello w\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nhjCd-AddhJQ",
        "outputId": "46942b48-1916-4a9c-b755-2f029bdf2afd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello wiiAEFf'2r!y,NRs ;X57yAdg}\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = train_data_t[:-1], train_data_t[1:]\n",
        "test_X, test_y = test_data_t[:-1], test_data_t[1:]\n",
        "\n",
        "def get_batch(X, y, idx, batch_size=16):\n",
        "  batch_x, batch_y = [], []\n",
        "  for i in range(batch_size):\n",
        "    batch_x.append(X[idx*batch_size+i  :idx*batch_size+i+context_window_size])\n",
        "    batch_y.append(y[idx*batch_size+i+1:idx*batch_size+i+context_window_size+1])\n",
        "\n",
        "  return torch.stack(batch_x), torch.stack(batch_y)"
      ],
      "metadata": {
        "id": "rPAGeQkjjeZn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YL_VrKlAz762"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# TODO: wrap in a function instead of duplicating code\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  test_loss = 0\n",
        "  for i in range(num_test_batches):\n",
        "    X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "    probs, loss = model(X, y)\n",
        "    test_loss += loss.item()\n",
        "  test_loss /= num_test_batches\n",
        "print(f\"random weight {test_loss=}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    X, y = get_batch(train_X, train_y, i, batch_size)\n",
        "    probs, loss = model(X, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  epoch_loss /= num_batches\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    for i in range(num_test_batches):\n",
        "      X, y = get_batch(test_X, test_y, i, batch_size)\n",
        "      probs, loss = model(X, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss /= num_test_batches\n",
        "\n",
        "  print(f\"{epoch=}, {epoch_loss=}; {test_loss=}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S63ht-ZIkT9T",
        "outputId": "dcad3643-8822-4f9a-9690-e2ecdfb94523"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random weight test_loss=4.431516471398951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:42<00:00, 101.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, epoch_loss=4.23637455629748; test_loss=4.225788273401481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:42<00:00, 102.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=1, epoch_loss=4.2212907846029415; test_loss=4.225944372093153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:41<00:00, 102.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=2, epoch_loss=4.221148357557696; test_loss=4.225657027222575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:42<00:00, 101.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=3, epoch_loss=4.221384235703668; test_loss=4.225590437963074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:41<00:00, 104.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=4, epoch_loss=4.219121327843777; test_loss=4.221889990680623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:42<00:00, 101.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=5, epoch_loss=4.218222948174144; test_loss=4.221361400196387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:42<00:00, 101.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=6, epoch_loss=4.218035585603048; test_loss=4.22091009431915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:41<00:00, 103.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=7, epoch_loss=4.217927774551303; test_loss=4.221243087600612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:41<00:00, 104.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=8, epoch_loss=4.217904448398324; test_loss=4.2212355841630655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4300/4300 [00:42<00:00, 100.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=9, epoch_loss=4.218024053906285; test_loss=4.221235454207446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\"Hello w\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9LUOOLzsqgjM",
        "outputId": "49674e39-8a8d-4922-9240-bc0a2d46b748"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello w                         '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num of trainable parameters in the model:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBAqE2XT0yl0",
        "outputId": "065710b2-c855-43e0-8325-5badc6147e36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5460"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJGAU5C1aHSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}